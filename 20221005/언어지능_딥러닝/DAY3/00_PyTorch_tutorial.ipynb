{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#PyTorch 튜토리얼 \n","\n","Tensor(텐서)는 Numpy의 다차원 행렬 자료구조인 ndarray와 유사하나 CPU 뿐만 아니라 GPU에서 실행될 수 있다는 차이점이 있습니다.\n","\n","PyTorch는 GPU와 CPU를 사용하는 딥러닝을 위한 최적화된 텐서 라이브러리입니다.\n","\n","본 코드에서 PyTorch를 사용하여 CNN, RNN 모델을 구현할 때 알아야할 기본적인 사항을 소개합니다. \n","\n","코드 출처: https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n","\n","PyTorch Documents: https://pytorch.org/docs/stable/index.html"],"metadata":{"id":"weHATskkbZV_"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"eu1VDs46bHzN","executionInfo":{"status":"ok","timestamp":1664946714421,"user_tz":-540,"elapsed":3166,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"outputs":[],"source":["# The torch package contains data structures for multi-dimensional tensors and \n","# defines mathematical operations over these tensors. \n","import torch\n","import numpy as np"]},{"cell_type":"markdown","source":["텐서 초기화하기"],"metadata":{"id":"MsQ8xnmtS1sO"}},{"cell_type":"code","source":["# 2차원 리스트 데이터로부터 텐서 생성\n","data = [[1, 2], [3, 4]]\n","# 텐서 생성\n","x_data = torch.tensor(data)\n","print(x_data)\n","print(x_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKydbeNnbXTY","outputId":"c85f3627-aece-4155-c1e3-a5ed9e3dda9b","executionInfo":{"status":"ok","timestamp":1664946714800,"user_tz":-540,"elapsed":382,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n","torch.Size([2, 2])\n"]}]},{"cell_type":"code","source":["# 랜덤값으로 텐서 생성. 정수값으로 텐서의 차원을 전달\n","# 랜덤값으로 채워진 (3, 4) 차원의 텐서를 생성 \n","tensor = torch.rand(3, 4)   \n","print (\"tensor :\", tensor)\n","print(f\"Shape of tensor: {tensor.shape}\")     \n","print(f\"Datatype of tensor: {tensor.dtype}\") \n","print(f\"Device tensor is stored on: {tensor.device}\") "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pr4iKqvkxrmu","outputId":"7a2b7bfd-1435-461c-84ff-1078794ee38a","executionInfo":{"status":"ok","timestamp":1664946715172,"user_tz":-540,"elapsed":381,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor : tensor([[9.1212e-01, 5.6359e-01, 4.5931e-04, 3.2841e-01],\n","        [8.6146e-01, 5.8954e-01, 7.1581e-01, 3.0010e-01],\n","        [4.4201e-01, 2.0217e-02, 6.2258e-01, 1.5160e-01]])\n","Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}]},{"cell_type":"code","source":["# Returns a tensor filled with the scalar value 1,with the shape defined by the argument size.\n","tensor = torch.ones(4, 4)\n","print(tensor)\n","# 모든 행에 대해서 1번열에 0값을 대입\n","tensor[:,1] = 0\n","print(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5E-stBHEyp9L","outputId":"57d8159a-1292-4480-e513-bd556c7371e7","executionInfo":{"status":"ok","timestamp":1664946715173,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}]},{"cell_type":"code","source":["# Concatenate : 텐서를 연결하기\n","# 딥러닝에서는 모델의 입력 또는 중간 연산 단계에서 두 개의 텐서를 연결하는 경우가 있습니다.\n","# 두 텐서를 연결해서 입력으로 사용하는 것은 두 텐서에 담긴 정보를 모두 사용한다는 의미입니다. \n","# dim : 텐서를 연결하여 어느 차원을 늘릴 것인지를 표시\n","t1 = torch.cat([tensor, tensor], dim=0) # 두 텐서를 연결하여 0번째 차원을 늘리라는 의미\n","print(\"tensor shape:\", tensor.shape)\n","print(\"->\", t1)\n","print(\"t1 shape:\", t1.shape) # 0번째 dimension이 늘어난 것을 확인\n","print(\"----------------------------\")\n","t2 = torch.cat([tensor, tensor], dim=1) # 두 텐서를 연결하여 1번째 차원을 늘리라는 의미\n","print(\"tensor shape:\", tensor.shape)\n","print(\"->\", t2)\n","print(\"t2 shape:\", t2.shape) # 1번째 dimension이 늘어난 것을 확인"],"metadata":{"id":"mlRH8Fzryt-3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664946715552,"user_tz":-540,"elapsed":2,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"0608ac0e-bf41-4474-b034-959523cd99dd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor shape: torch.Size([4, 4])\n","-> tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","t1 shape: torch.Size([8, 4])\n","----------------------------\n","tensor shape: torch.Size([4, 4])\n","-> tensor([[1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1.]])\n","t2 shape: torch.Size([4, 8])\n"]}]},{"cell_type":"markdown","source":["#신경망 모델 생성하기\n","\n","모든 신경망 클래스는 torch.nn 패키지를 통해서 생성합니다.\n","\n","torch.nn.Module은 PyTorch의 모든 신경망의 Base Class이며 새로운 신경망 모델은 torch.nn.Module 클래스를 상속하여 정의해야 합니다.\n","\n","새로운 클래스 내에서 \\__init()__함수와 forward()함수를 반드시 override 해야 합니다.\n","\n","\\__init()__함수에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function(ReLU 등)를 정의합니다. \n","\n","forward()에서는 모델에서 실행되어야 하는 연산을 정의합니다.\n","\n","backward 연산은 backward() 함수를 호출하면 PyTorch가 자동으로 수행하므로 forward()만 정의합니다.\n","\n","forward()에서는 input 데이터에 대해 어떤 연산을 진행하여 output이 나올지를 정의해 주는 것입니다. "],"metadata":{"id":"MFaz5onfc3V9"}},{"cell_type":"code","source":["import torch.nn as nn           # 신경망 구현을 위한 데이터 구조, 신경망 레이어 등이 정의되어 있음 \n","import torch.nn.functional as F # Convolution, Pooling, Activation, Linear 함수 등이 정의되어 있음"],"metadata":{"id":"6Qwmr5Lxcurf","executionInfo":{"status":"ok","timestamp":1664946716337,"user_tz":-540,"elapsed":6,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# torch.nn.Module은 PyTorch의 모든 신경망의 Base Class\n","# __init()__과 forward()를 반드시 override 해야 한다.\n","class Net(nn.Module):\n","  def __init__(self): # init과 foward 함수 꼭 재정의를 해주어야 함\n","    super(Net, self).__init__()\n","    # 입력 이미지 채널 1개, 출력 채널 6개, 5x5의 정사각 컨볼루션 행렬\n","    # 컨볼루션 커널 정의\n","    # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n","    self.conv1 = nn.Conv2d(1, 6, 5)   # 입력 채널 크기, 출력 채널 크기, 커널 크기\n","    self.conv2 = nn.Conv2d(6, 16, 5)  # 입력 채널 크기, 출력 채널 크기, 커널 크기\n","    \n","    # Fully Connected Layer: y = Wx + b\n","    self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5은 이미지 차원에 해당\n","    self.fc2 = nn.Linear(120, 60) # 120개의 차원을 60개의 차원으로 계산\n","    self.fc3 = nn.Linear(60, 10)  # 60 -> 10개 차원으로 계산\n","\n","  def forward(self, x):\n","    # convolution을 거치게 되면 이미지의 크기는 kernel - 1만큼 감소함\n","    # 현재 입력되는 이미지의 크기가 32 X 32\n","    # conv1을 통해 출력되는 이미지의 크기는 (32 - 5 + 1) X (32 - 5 + 1) = 28 X 28\n","    # (2, 2) 크기 윈도우에 대해 맥스 풀링 -> 이미지의 크기는 2분의 1이 되므로 -> 최종 출력 이미지 크기는 14 X 14\n","    # torch.nn.functional.max_pool2d(input, kernel_size)\n","    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # [batch size, 채널 크기, 이미지 가로 크기, 이미지 세로 크기] == [1, 6, 14, 14] \n","\n","    # conv2을 통해 출력되는 이미지의 크기는 (14 - 5 + 1) X (14 - 5 + 1) = 10 X 10\n","    # (2, 2) 크기 윈도우에 대해 맥스 풀링 -> 이미지의 크기는 2분의 1이 되므로 -> 최종 출력 이미지 크기는 5 X 5\n","    # 크기가 제곱수라면, 하나의 숫자만을 특정(specify)\n","    x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) # [1, 16, 5, 5]\n","    # torch.flatten(input, start_dim, end_dim) : flattens input by reshaping it into a one-dimensional tensor. \n","    x = torch.flatten(x, 1) # (tensor, dimension) batch 차원을 제외한 모든 차원을 하나로 평탄화(flatten) [1, 16 * 5 * 5] 들어가는 input의 차원은 16*5*5 = 400\n","    x = F.relu(self.fc1(x)) # [1, 120]\n","    x = F.relu(self.fc2(x)) # [1, 60]\n","    x = self.fc3(x) # [1, 10]\n","    return x"],"metadata":{"id":"Jr5g3dg0c9nv","executionInfo":{"status":"ok","timestamp":1664946716673,"user_tz":-540,"elapsed":2,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["net = Net()\n","print(net) # 신경망 모델의 구조"],"metadata":{"id":"OS3iirUStWXR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"28ba6ed3-6358-417d-8760-9c937e272cf4","executionInfo":{"status":"ok","timestamp":1664946717091,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=60, bias=True)\n","  (fc3): Linear(in_features=60, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["input = torch.randn(1, 1, 32, 32) # [batch size, 입력 채널 크기, 가로 길이, 세로 길이] -> 입력데이터\n","out = net(input)\n","print(\"out shape:\", out.shape)\n","print(out)  # out shape: (1, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Rt3x6jpdHw5","outputId":"ef504ddf-b74c-4100-d07e-8385e7099a75","executionInfo":{"status":"ok","timestamp":1664946717854,"user_tz":-540,"elapsed":368,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["out shape: torch.Size([1, 10])\n","tensor([[-0.0232, -0.0278, -0.0129,  0.0035,  0.0384, -0.0901,  0.0932,  0.0244,\n","         -0.1483, -0.1132]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["# view 함수 : reshape 함수. 텐서의 형태(Shape)를 변경함. 변경 전과 후에 원소의 갯수는 유지되어야 한다.\n","# view(1, -1) : 첫번째 차원은 1이 되도록 하되,-1로 표시된 2번째 차원은 파이토치가 알아서 계산하라는 의미\n","output = net(input)               # 입력 데이터를 신경망 모델에 전달하여 예측값을 얻음 shape: (1, 10)\n","print(\"output shape:\", output.shape)\n","target = torch.randn(10)          # 임의의 텐서를 생성하여 정답값으로 가정\n","print (\"target shape:\", target.shape)\n","target = target.view(1, -1)       # 모델의 출력 텐서와 동일한 shape로 변경 : (1, 10)\n","                                  # 정답값으로 가정한 target도 output 차원과 동일하게 변경 view(1, -1) => 0번째는 1주고 1번째는 알맞게 넣어라\n","print (\"after reshape(view) -> target shape:\", target.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nH6NhU2EdZ39","outputId":"2e14750f-dca4-4977-8beb-a84b1242ff1c","executionInfo":{"status":"ok","timestamp":1664946717855,"user_tz":-540,"elapsed":6,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["output shape: torch.Size([1, 10])\n","target shape: torch.Size([10])\n","after reshape(view) -> target shape: torch.Size([1, 10])\n"]}]},{"cell_type":"markdown","source":["손실함수 (Loss Function) 설정"],"metadata":{"id":"D1M56m1Joo4p"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","# 예측값과 정답값 사이의 오차를 줄이면서 최적화해 나감\n","# 어떤 텐서가 학습에 필요한 텐서라면 backpropagation을 통하여 gradient를 구해야 합니다.\n","# 텐서의 옵션 requireds_grad를 True로 설정하면 텐서에 실행되는 모든 연산들을 트랙킹하여 자동으로 gradient를 계산합니다.\n","input = torch.randn(3, 5, requires_grad=True) \n","target = torch.randn(3, 5)\n"," # 손실함수로 MSE(Mean Squared Error)를 설정\n"," # 평균제곱오차(MSE)는 오차를 제곱한 값의 평균. 오차란 모델이 예측한 값과 실제 정답과의 차이\n","criterion = nn.MSELoss()          \n","output = criterion(input, target) # Loss 계산\n","output.backward() # 역전파 진행\n","print('input: ', input)\n","print('target: ', target)\n","print('output: ', output) # 임의의 실수값을 가진 tensor\n","\n","# 손실함수 설정 시 nn에서 어떤 함수를 불러와서 loss를 계산하는 상황"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OF9WEhcIoidP","executionInfo":{"status":"ok","timestamp":1664946718963,"user_tz":-540,"elapsed":8,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"0e5c26a6-ef2c-4b7a-c6f2-3086382310b0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["input:  tensor([[ 0.4468, -0.6449,  0.7325,  1.3127, -0.1208],\n","        [ 1.1454,  0.2387, -0.6937, -0.0083,  1.1390],\n","        [-0.6047,  0.2860, -0.8666, -0.4307,  1.9010]], requires_grad=True)\n","target:  tensor([[-0.4074,  0.2900, -0.8044,  0.7536, -0.8668],\n","        [-0.5720, -0.9828,  0.7305,  2.5481, -1.3244],\n","        [-1.1969,  0.3493,  0.8787,  0.2553, -0.2976]])\n","output:  tensor(2.1742, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["옵티마이저 설정"],"metadata":{"id":"1el6Hg6D5_xN"}},{"cell_type":"code","source":["# torch.optim : 신경망 학습을 위한 파라미터 최적화 알고리즘이 구현되어 있는 클래스\n","import torch.optim as optim\n","# Adam Optimizer 객체 생성\n","optimizer = optim.Adam(net.parameters(), lr=0.01) # adam 옵티마이저 객체 생성 : 학습가능한 파라미터들을 넘겨줌(net 우리가 정의한 신경망을 담고 있음), 학습률\n","\n","# 학습 과정(training loop)\n","# Pytorch에서는 gradients 값들을 backward를 할 때 계속 누적하기 때문에 \n","# 학습을 시작하기 전에 gradients 버퍼를 zero로 reset해야 한다.\n","optimizer.zero_grad()\n","\n","input = torch.randn(1, 1, 32, 32)\n","output = net(input)               # 이 클래스에 input 전달한 순간에 foward 실행됨 => conV 등등 연산이 진행\n","target = torch.randn(10)          # 예시를 위한 임의의 정답\n","target = target.view(1, -1)       # 출력과 같은 shape로 만듬\n","loss = criterion(output, target)  # Loss 계산\n","print(loss)\n","\n","loss.backward()         # 역전파 함수 실행을 통해 gradient 계산\n","optimizer.step()        # gradient를 기반으로 실제 파라미터 업데이트를 실행"],"metadata":{"id":"dNH1E4BbeFP3","executionInfo":{"status":"ok","timestamp":1664946719770,"user_tz":-540,"elapsed":6,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b3a04d7-7a1f-497f-a4c9-22c66c5b0a7a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.8221, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"code","source":["# torch.optim : 신경망 학습을 위한 파라미터 최적화 알고리즘들이 구현되어 있는 팩키지\n","import torch.optim as optim\n","# Optimizer 객체를 생성하고 모델의 파라미터를 전달, learning rate 설정\n","optimizer = optim.Adam(net.parameters(), lr=0.01) \n","\n","# 학습 과정(training loop)\n","# Pytorch에서는 gradients 값들을 backward를 할 때 계속 누적하기 때문에 \n","# 학습을 시작하기 전에 gradients 버퍼를 zero로 reset해야 한다.\n","optimizer.zero_grad()\n","\n","input = torch.randn(1, 1, 32, 32)\n","output = net(input)\n","target = torch.randn(10)          # 예시를 위한 임의의 정답\n","target = target.view(1, -1)       # 출력과 같은 shape로 만듬\n","loss = criterion(output, target)  # Loss 계산\n","print(loss)\n","\n","loss.backward()         # 역전파 함수 실행을 통해 gradient 계산\n","optimizer.step()        # 파라미터 업데이트를 실행"],"metadata":{"executionInfo":{"status":"ok","timestamp":1664946720056,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a45cbe3f-20fc-4b98-eb77-fe4160ed2058","id":"2lZ1-34tX2yW"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9062, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["GPU 가속 이용하기"],"metadata":{"id":"kHymma7IcdaH"}},{"cell_type":"code","source":["# 현재 개발환경에서 GPU 가속이 가능한지 확인\n","print(torch.cuda.is_available()) #  True => GPU를 사용할 수 있는 환경"],"metadata":{"id":"_rNJ7nfhcfbS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664946722862,"user_tz":-540,"elapsed":4,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"0709773c-a6f9-4d60-8684-a5801568871c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# GPU 사용이 가능하면 cuda로 연산하도록 device를 설정 그렇지 않으면 cpu로 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","net.to(device) # 이용가능한 device에서 적용"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuZAiUbhcnk6","outputId":"4ecc69b6-26e3-4820-afca-17d892a87752","executionInfo":{"status":"ok","timestamp":1664946822624,"user_tz":-540,"elapsed":4650,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=60, bias=True)\n","  (fc3): Linear(in_features=60, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["PyTorch TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","데이터셋을 샘플링하고, 학습 루프를 편리하게 구현하기 위한 함수"],"metadata":{"id":"ZMNF4FcTgHX4"}},{"cell_type":"code","source":["# 랜덤한 학습 데이터 생성\n","train_inputs = torch.randn(100, 32, 128) # [전체 데이터 개수, 데이터의 최대 길이, 히든 벡터 크기]\n","train_labels = torch.randn(100, 3) # [전체 데이터 개수, 예측할 클래스 개수]\n"],"metadata":{"id":"0hQGB9FQgFmG","executionInfo":{"status":"ok","timestamp":1664946843762,"user_tz":-540,"elapsed":420,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Dataset에 필요한 패키지 임포트\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"metadata":{"id":"dTqLJd41gZ8g","executionInfo":{"status":"ok","timestamp":1664946844084,"user_tz":-540,"elapsed":2,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["batch_size = 12 # 매 학습 Step 마다 샘플링할 데이터 개수\n","\n","train_data = TensorDataset(train_inputs, train_labels) # 학습데이터와 레이블을 하나의 TensorDataset으로 결합 가능\n","train_sampler = RandomSampler(train_data) # 데이터를 샘플링 할 함수(순차적으로 뽑아올지, 랜덤하게 뽑아올지) => 데이터를 학습 시 랜덤하게 하는 것\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # 학습 시 미니배치 단위로 데이터를 자동 로딩 "],"metadata":{"id":"izHGZVsmgXda","executionInfo":{"status":"ok","timestamp":1664947328813,"user_tz":-540,"elapsed":291,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# dataloader를 통해서 학습 루프 생성\n","for step, batch in enumerate(train_dataloader): # enumerate : 인덱스(index)와 데이터값에 동시에 접근 / batch 사이즈 만큼 배치에 데이터를 로드\n","  x_data, x_label = batch\n","  # 데이터 로딩 step \n","  print(step, x_data.shape, x_label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so6pZGaEg5qI","outputId":"6de2a7df-ef1b-4fbc-d01c-8d6b4a0dd06c","executionInfo":{"status":"ok","timestamp":1664947329192,"user_tz":-540,"elapsed":4,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["0 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","1 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","2 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","3 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","4 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","5 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","6 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","7 torch.Size([12, 32, 128]) torch.Size([12, 3])\n","8 torch.Size([4, 32, 128]) torch.Size([4, 3])\n"]}]},{"cell_type":"code","source":["# 마지막 9번째는 배치사이즈 12개를 못채워서 4개만"],"metadata":{"id":"JgXtQhyYkLAA"},"execution_count":null,"outputs":[]}]}