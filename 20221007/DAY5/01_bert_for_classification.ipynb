{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"95a262797ffa4adf9cf10e114dd2d2fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_328a23eeb4dd4d14a71510715d7c2c80","IPY_MODEL_e8017a4be2e9426b91acdb8fa1909014","IPY_MODEL_648ca7ea32964eecb8a30c1615ee2be5"],"layout":"IPY_MODEL_a018df89b744499185bedb08ecb5fb6c"}},"328a23eeb4dd4d14a71510715d7c2c80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d393a67690641f7a4d28cdc896401d5","placeholder":"​","style":"IPY_MODEL_aa33ce6e2d8f4dec86eaaa5aeecea419","value":"Downloading: 100%"}},"e8017a4be2e9426b91acdb8fa1909014":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24043d03fb5c43b0b49187213ed1e3fd","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5d7b036a41549319d0a02161b0b6c35","value":231508}},"648ca7ea32964eecb8a30c1615ee2be5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d25f65cb81d84f8eb8f63929abe91a97","placeholder":"​","style":"IPY_MODEL_5a8f2564f0ce46e5a0b1634cba3b578b","value":" 232k/232k [00:00&lt;00:00, 255kB/s]"}},"a018df89b744499185bedb08ecb5fb6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d393a67690641f7a4d28cdc896401d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa33ce6e2d8f4dec86eaaa5aeecea419":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24043d03fb5c43b0b49187213ed1e3fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5d7b036a41549319d0a02161b0b6c35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d25f65cb81d84f8eb8f63929abe91a97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a8f2564f0ce46e5a0b1634cba3b578b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e390f493d7e4125a76b9f3ecc742191":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40f605478f804ad086323f7f0866afe3","IPY_MODEL_ccf62849dea74e70884adec1c2f5e03c","IPY_MODEL_839d7bf62a6f4903a9ee3d068e63044c"],"layout":"IPY_MODEL_9e2fc9695d9f47a6bc9cf231101cbabe"}},"40f605478f804ad086323f7f0866afe3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4049709d0890409bbead0c32dec99b7b","placeholder":"​","style":"IPY_MODEL_04ad5d7b4d7b44c7983db5f57ef72973","value":"Downloading: 100%"}},"ccf62849dea74e70884adec1c2f5e03c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_789878955f524726a47bc03d4d1161c5","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5377bb4b3984812a25c9f5950cc6e14","value":28}},"839d7bf62a6f4903a9ee3d068e63044c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce81978c3f294fbaa4acdff75a0ad10e","placeholder":"​","style":"IPY_MODEL_67eeb1179a3444eca2eb2394839e1c87","value":" 28.0/28.0 [00:00&lt;00:00, 837B/s]"}},"9e2fc9695d9f47a6bc9cf231101cbabe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4049709d0890409bbead0c32dec99b7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04ad5d7b4d7b44c7983db5f57ef72973":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"789878955f524726a47bc03d4d1161c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5377bb4b3984812a25c9f5950cc6e14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce81978c3f294fbaa4acdff75a0ad10e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67eeb1179a3444eca2eb2394839e1c87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"118f9cef17bd4994bdfa39af329e5adf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d35639dcf1440caaf99ccc54f828e6d","IPY_MODEL_26bd74041e204c6089aae407bb56af14","IPY_MODEL_1a82995566dc409ba11e08271039d337"],"layout":"IPY_MODEL_7ece9adf499d46fea7620029bdffdc64"}},"9d35639dcf1440caaf99ccc54f828e6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_444edc72684e457484e39130b4d3ed65","placeholder":"​","style":"IPY_MODEL_f0a800acc2a24636ac10c995b71ee51b","value":"Downloading: 100%"}},"26bd74041e204c6089aae407bb56af14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6b1084adce94fb7893451ec003f1776","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e236c074025844d0aaadca06ee4fdcd5","value":570}},"1a82995566dc409ba11e08271039d337":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61df75c30ec74c7a9a852178f7f1301a","placeholder":"​","style":"IPY_MODEL_484f416e61b94a88b3a610ddff19733c","value":" 570/570 [00:00&lt;00:00, 17.8kB/s]"}},"7ece9adf499d46fea7620029bdffdc64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"444edc72684e457484e39130b4d3ed65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0a800acc2a24636ac10c995b71ee51b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6b1084adce94fb7893451ec003f1776":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e236c074025844d0aaadca06ee4fdcd5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61df75c30ec74c7a9a852178f7f1301a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"484f416e61b94a88b3a610ddff19733c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58f616575f294c20b1652f6bff37b6b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a146ad3ab92e426ba396da1c48850211","IPY_MODEL_ac4acd3bd8024237969f39377bce9015","IPY_MODEL_033e3c9111684ad1989d45dd5e6e2635"],"layout":"IPY_MODEL_7ab5f36206174d7e8577d45a370302bd"}},"a146ad3ab92e426ba396da1c48850211":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b5b58ed9524bf48f2d17d2c0715c2d","placeholder":"​","style":"IPY_MODEL_42e1e0ca254d4df7a55aadf8db0c56cc","value":"Downloading: 100%"}},"ac4acd3bd8024237969f39377bce9015":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19f8919d27ea468f90444aa3cff409b4","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_466e30cc190649928deef9dab1f77d2b","value":440473133}},"033e3c9111684ad1989d45dd5e6e2635":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0ab6b57d9a5430db93cd55236708f50","placeholder":"​","style":"IPY_MODEL_7a18c7b40bd54ad89463fefed9824d1f","value":" 440M/440M [00:07&lt;00:00, 59.3MB/s]"}},"7ab5f36206174d7e8577d45a370302bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46b5b58ed9524bf48f2d17d2c0715c2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42e1e0ca254d4df7a55aadf8db0c56cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19f8919d27ea468f90444aa3cff409b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"466e30cc190649928deef9dab1f77d2b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0ab6b57d9a5430db93cd55236708f50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a18c7b40bd54ad89463fefed9824d1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"MWmuv8wwoi5X"},"source":["# BERT Fine-Tuning Tutorial with PyTorch\n","\n","코드&설명 제공 (Thanks to)\n","- 이영준 (2020 KEPCO 문제해결형 인공지능기술개발교육 TA)\n","\n","본 실습에서는 `huggingface` PyTorch 라이브러리를 이용하여 sentiment classification에서 BERT를 어떻게 fine-tuning 하는지에 대한 부분을 설명합니다. 또한, BERT 모델을 fine-tuning 하기 위해 amazon review 데이터(영어)를 사용합니다. \n","\n","## Documentation\n","\n","본 실습 자료를 위해 참고한 자료는 아래와 같습니다.\n","\n","- Chris McCormick's tutorial: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n","- huggingface github: https://github.com/huggingface/transformers\n","- huggingface documentation: https://huggingface.co/transformers/\n","- 딥러닝을 이용한 자연어처리 입문: https://wikidocs.net/115055"]},{"cell_type":"markdown","metadata":{"id":"5tZo8B6eVJFf"},"source":["## Step 0: Connect to Google drive\n","\n"]},{"cell_type":"code","metadata":{"id":"omfN5AZ9ys_g","outputId":"e8293e3e-9d83-46f2-dc76-bd2aef0c98f6","executionInfo":{"status":"ok","timestamp":1665120976403,"user_tz":-540,"elapsed":24586,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"6QhEGRaVVaFh"},"source":["## Step 1: Import modules"]},{"cell_type":"code","metadata":{"id":"aYhj-RIeybIm","outputId":"72443b71-e38c-491e-e3ac-644d2c1bfc50","executionInfo":{"status":"ok","timestamp":1665120978504,"user_tz":-540,"elapsed":2105,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import pickle as pc\n","import os\n","import numpy as np\n","# csv 모듈은 CSV 형식(쉼표로 구분된 표 형식) 데이터를 읽고 쓰는 클래스를 제공\n","import csv\n","import torch\n","\n","# torch 버전 확인\n","print(\"Pytorch Version: \", torch.__version__)\n","\n","# GPU 사용 가능한지 여부 확인\n","if torch.cuda.is_available():\n","    \n","    # PyTorch 에게 GPU 사용할거라고 알려주기\n","    device = torch.device(\"cuda\")\n","    \n","    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n","    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n","else:\n","    print(\"No GPU available, using the CPU instead.\")\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch Version:  1.12.1+cu113\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"markdown","metadata":{"id":"9DoBvLNQV3wj"},"source":["## Step 2: Installing the Hugging Face Library\n","* Hugging Face는 언어모델 공유 플랫폼&커뮤니티 (https://github.com/huggingface) \n","\n","* 논문으로 발표된 다양한 Transformer 기반의 모델이 구현되어 있음\n","\n","* 사용자들은 자신이 구축한 언어모델을 Huggingface의 모델 Hub를 통해서 손쉽게 공유할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"ZrMn70gSV3Ko","outputId":"f6a5ab2a-05ec-4a0a-eaef-3261047fbe0f","executionInfo":{"status":"ok","timestamp":1665120987589,"user_tz":-540,"elapsed":9090,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# transformers 팩키지 설치\n","!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 38.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 60.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 70.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"g1wW8UouVh8M"},"source":["## Step 3: Configure the experiments\n","\n","- 모델의 실험(학습 및 평가)을 위해 필요한 파라미터 및 인자 설정\n","    - Hyperparameter: hidden unit size, vocabulary size, max length, dropout rate 등\n","    - Argument: file directory 등\n","\n","*Note: 해당 실습 자료에서는 데이터 경로만 지정*"]},{"cell_type":"code","metadata":{"id":"s_4QT4BDybIq","executionInfo":{"status":"ok","timestamp":1665121085124,"user_tz":-540,"elapsed":479,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["train_filename = '/content/gdrive/My Drive/Colab Notebooks/aivle/data/amazon/bert_train_data_all.csv'\n","test_data = '/content/gdrive/My Drive/Colab Notebooks/aivle/data/amazon/bert_balanced_data'\n","test_label = '/content/gdrive/My Drive/Colab Notebooks/aivle/data/amazon/bert_balanced_label'"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kRSvzQwfWFzo"},"source":["## Step 4: Load Amazon Review Dataset\n","\n","- `load_data`: amazon review data 에서 **[reviewText, label]** 형태로 데이터를 불러오는 함수"]},{"cell_type":"code","metadata":{"id":"1In5hQtPybIs","executionInfo":{"status":"ok","timestamp":1665121088993,"user_tz":-540,"elapsed":493,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["def load_data(filename):\n","    data = list()\n","    label = list()\n","    \n","    f = open(filename, 'r', encoding='utf-8')\n","    reader = csv.reader(f)\n","    for idx, line in enumerate(reader):\n","        if idx == 0:\n","            continue\n","        # line[2]에 label 1(긍정), 0(부정) line[5]에 review text가 있음 \n","        data.append(line[5])\n","        label.append(int(line[2]))\n","\n","    f.close() \n","   \n","    # data와 label 사이즈 일치 여부 확인\n","    assert len(data) == len(label)  \n","    return data, label"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"USBIeV1TybIv","executionInfo":{"status":"ok","timestamp":1665121092016,"user_tz":-540,"elapsed":2091,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["# train_data : 리뷰문장 (text), train_lable : 1(긍정) 또는 0(부정)\n","train_data, train_label = load_data(train_filename)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"43O3FuU7ybI0","outputId":"bef42b12-a7b1-4c09-85ef-dce620602a71","executionInfo":{"status":"ok","timestamp":1665121092017,"user_tz":-540,"elapsed":10,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["print(\"Size of train data: {}\".format(len(train_data)))\n","print(\"Size of train label: {}\".format(len(train_label)))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of train data: 10727\n","Size of train label: 10727\n"]}]},{"cell_type":"markdown","metadata":{"id":"23USqxBAybJA"},"source":["## Step 5: Tokenization & Input Formatting\n","\n","본 단계에서는 BERT 가 학습한 format 에 맞게 amazon review dataset 을 변환해줍니다. "]},{"cell_type":"markdown","metadata":{"id":"l00cRz_EybJC"},"source":["### Step 5-1: BERT Tokenizer\n","\n","BERT 모델에 text 를 입력으로 주기 위해서는 BERT 에서 사용한 tokenizer 를 이용하여 text 를 token 단위로 나누고, 각 token 들을 특정 index 로 mapping 시켜줍니다. \n","\n","- `BertTokenizer`: punctuation splitting + wordpiece\n","  - `bert-base-uncased`: 12-layer, 768-hidden size, 12-heads, 110M parameters. Trained on **lower-cased** English text.\n","  - uncased는 대문자를 모두 소문자로 바꾼 후 토크나이즈하는 것을 의미 (vs. cased)"]},{"cell_type":"code","metadata":{"id":"Kks51eZ9ybJD","outputId":"bfb53281-1e71-4e22-c9ca-5e49ffbbca52","executionInfo":{"status":"ok","timestamp":1665121330194,"user_tz":-540,"elapsed":9191,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["95a262797ffa4adf9cf10e114dd2d2fa","328a23eeb4dd4d14a71510715d7c2c80","e8017a4be2e9426b91acdb8fa1909014","648ca7ea32964eecb8a30c1615ee2be5","a018df89b744499185bedb08ecb5fb6c","2d393a67690641f7a4d28cdc896401d5","aa33ce6e2d8f4dec86eaaa5aeecea419","24043d03fb5c43b0b49187213ed1e3fd","c5d7b036a41549319d0a02161b0b6c35","d25f65cb81d84f8eb8f63929abe91a97","5a8f2564f0ce46e5a0b1634cba3b578b","1e390f493d7e4125a76b9f3ecc742191","40f605478f804ad086323f7f0866afe3","ccf62849dea74e70884adec1c2f5e03c","839d7bf62a6f4903a9ee3d068e63044c","9e2fc9695d9f47a6bc9cf231101cbabe","4049709d0890409bbead0c32dec99b7b","04ad5d7b4d7b44c7983db5f57ef72973","789878955f524726a47bc03d4d1161c5","d5377bb4b3984812a25c9f5950cc6e14","ce81978c3f294fbaa4acdff75a0ad10e","67eeb1179a3444eca2eb2394839e1c87","118f9cef17bd4994bdfa39af329e5adf","9d35639dcf1440caaf99ccc54f828e6d","26bd74041e204c6089aae407bb56af14","1a82995566dc409ba11e08271039d337","7ece9adf499d46fea7620029bdffdc64","444edc72684e457484e39130b4d3ed65","f0a800acc2a24636ac10c995b71ee51b","f6b1084adce94fb7893451ec003f1776","e236c074025844d0aaadca06ee4fdcd5","61df75c30ec74c7a9a852178f7f1301a","484f416e61b94a88b3a610ddff19733c"]}},"source":["from transformers import BertTokenizer\n","\n","# BERT tokenizer 불러오기\n","# do_lower_case : True이면 모두 소문자로 변환, False이면 대소문자 구분\n","print(\"Loading BERT tokenizer...\")\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a262797ffa4adf9cf10e114dd2d2fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e390f493d7e4125a76b9f3ecc742191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"118f9cef17bd4994bdfa39af329e5adf"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"VTwSHYz1ybJG","outputId":"96071b83-0a1e-4643-cf2e-b9dd2d12dea9","executionInfo":{"status":"ok","timestamp":1665121330195,"user_tz":-540,"elapsed":25,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# 하나의 sentence에 대해 BertTokenizer 적용\n","\n","# Print the original sentence.\n","print(\"Original: \", train_data[0])\n","print(\"Original: \", train_data[1])\n","print()\n","\n","# Print the sentence split into tokens.\n","print(\"Tokenized: \", tokenizer.tokenize(train_data[0]))\n","print(\"Tokenized: \", tokenizer.tokenize(train_data[1]))\n","print()\n","\n","# Print the sentence mapped to token ids.\n","print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[0])))\n","print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[1])))\n","print()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  I never thought that I would find the right moisturizer for my skin. I have combination skin with large pores that get gunked up if I neglect them and I ve struggled for some time to find a product that would provide sufficient moisture without breaking me out or turning my face into an oil-slick. This is truly a holy grail moisturizer for me and has a nice face feel and natural herbal scent. Will definitely repurchase!  I m a 30 something multi-ethnic muxer and a recent convert to korean beauty. This is my new routine and my skin has never felt or looked bettter!  1) Banila Co Clean It Zero Reservatrol 2) Neogen Green Tea Real Fresh Foam Cleanser 3) MISSHA Time Revolution First Treatment Essence (Morning) or MISSHA Time Revolution Night Repair New Science Ampoule (Night) 4) Cosrx Oil-Free Ultra-Moisturizing Lotion (Morning and ever OTHER night) or Cosrx Ultimate Nourishing Rice Overnight Mask\n","Original:  I started using Kbeauty products after becoming frustrated last year with my increasingly expensive American products that seemed to become less effect as time went on. I found quite a few products that work extremely well by Cosrx  this being one of them. Now as hard as it may be to find the right skincare finding the right moisturizer seems to be a never ending battle no matter what other miracle products may be sitting in your bathroom. I was hesitant to purchase this at first due to the price  & described scent. However the size which is pretty large helps the price & the birch sap smell (not fragrance) that was worrying me actually became a huge bonus! I get horrible headaches/ breakouts if I use anything with a scent  but I do miss being able to use those products so this is the perfect balance for me. As for application I started out using 4-5 pumps  way too much! Less is more  it can lead to a greasy look  however the product isn t actually greasy just concentrated. Now that it s summer I use one pump on my middle finger & rub it between my fingers and then tap it into my face. I ve also applied it with a silicone brush with great results. The bottle again is not tiny so keep that in mind when looking at the price  this is going to last you quite a while. Another Cosrx product that knocks it out of the park  no frills/ misleading marketing messages  just conscious packaging & ingredients. Try the rice mask for night as well  however try & order in the tube it s more hygienic. I ve attached a photo for scale.\n","\n","Tokenized:  ['i', 'never', 'thought', 'that', 'i', 'would', 'find', 'the', 'right', 'moist', '##uri', '##zer', 'for', 'my', 'skin', '.', 'i', 'have', 'combination', 'skin', 'with', 'large', 'por', '##es', 'that', 'get', 'gun', '##ked', 'up', 'if', 'i', 'neglect', 'them', 'and', 'i', 've', 'struggled', 'for', 'some', 'time', 'to', 'find', 'a', 'product', 'that', 'would', 'provide', 'sufficient', 'moisture', 'without', 'breaking', 'me', 'out', 'or', 'turning', 'my', 'face', 'into', 'an', 'oil', '-', 'slick', '.', 'this', 'is', 'truly', 'a', 'holy', 'gr', '##ail', 'moist', '##uri', '##zer', 'for', 'me', 'and', 'has', 'a', 'nice', 'face', 'feel', 'and', 'natural', 'herbal', 'scent', '.', 'will', 'definitely', 'rep', '##ur', '##chase', '!', 'i', 'm', 'a', '30', 'something', 'multi', '-', 'ethnic', 'mu', '##x', '##er', 'and', 'a', 'recent', 'convert', 'to', 'korean', 'beauty', '.', 'this', 'is', 'my', 'new', 'routine', 'and', 'my', 'skin', 'has', 'never', 'felt', 'or', 'looked', 'bet', '##tter', '!', '1', ')', 'ban', '##ila', 'co', 'clean', 'it', 'zero', 'res', '##er', '##vat', '##rol', '2', ')', 'neo', '##gen', 'green', 'tea', 'real', 'fresh', 'foam', 'clean', '##ser', '3', ')', 'miss', '##ha', 'time', 'revolution', 'first', 'treatment', 'essence', '(', 'morning', ')', 'or', 'miss', '##ha', 'time', 'revolution', 'night', 'repair', 'new', 'science', 'amp', '##ou', '##le', '(', 'night', ')', '4', ')', 'co', '##sr', '##x', 'oil', '-', 'free', 'ultra', '-', 'moist', '##uri', '##zing', 'lot', '##ion', '(', 'morning', 'and', 'ever', 'other', 'night', ')', 'or', 'co', '##sr', '##x', 'ultimate', 'no', '##uri', '##shing', 'rice', 'overnight', 'mask']\n","Tokenized:  ['i', 'started', 'using', 'kb', '##eau', '##ty', 'products', 'after', 'becoming', 'frustrated', 'last', 'year', 'with', 'my', 'increasingly', 'expensive', 'american', 'products', 'that', 'seemed', 'to', 'become', 'less', 'effect', 'as', 'time', 'went', 'on', '.', 'i', 'found', 'quite', 'a', 'few', 'products', 'that', 'work', 'extremely', 'well', 'by', 'co', '##sr', '##x', 'this', 'being', 'one', 'of', 'them', '.', 'now', 'as', 'hard', 'as', 'it', 'may', 'be', 'to', 'find', 'the', 'right', 'skin', '##care', 'finding', 'the', 'right', 'moist', '##uri', '##zer', 'seems', 'to', 'be', 'a', 'never', 'ending', 'battle', 'no', 'matter', 'what', 'other', 'miracle', 'products', 'may', 'be', 'sitting', 'in', 'your', 'bathroom', '.', 'i', 'was', 'hesitant', 'to', 'purchase', 'this', 'at', 'first', 'due', 'to', 'the', 'price', '&', 'described', 'scent', '.', 'however', 'the', 'size', 'which', 'is', 'pretty', 'large', 'helps', 'the', 'price', '&', 'the', 'birch', 'sap', 'smell', '(', 'not', 'fragrance', ')', 'that', 'was', 'worrying', 'me', 'actually', 'became', 'a', 'huge', 'bonus', '!', 'i', 'get', 'horrible', 'headache', '##s', '/', 'breakout', '##s', 'if', 'i', 'use', 'anything', 'with', 'a', 'scent', 'but', 'i', 'do', 'miss', 'being', 'able', 'to', 'use', 'those', 'products', 'so', 'this', 'is', 'the', 'perfect', 'balance', 'for', 'me', '.', 'as', 'for', 'application', 'i', 'started', 'out', 'using', '4', '-', '5', 'pumps', 'way', 'too', 'much', '!', 'less', 'is', 'more', 'it', 'can', 'lead', 'to', 'a', 'greasy', 'look', 'however', 'the', 'product', 'isn', 't', 'actually', 'greasy', 'just', 'concentrated', '.', 'now', 'that', 'it', 's', 'summer', 'i', 'use', 'one', 'pump', 'on', 'my', 'middle', 'finger', '&', 'rub', 'it', 'between', 'my', 'fingers', 'and', 'then', 'tap', 'it', 'into', 'my', 'face', '.', 'i', 've', 'also', 'applied', 'it', 'with', 'a', 'silicon', '##e', 'brush', 'with', 'great', 'results', '.', 'the', 'bottle', 'again', 'is', 'not', 'tiny', 'so', 'keep', 'that', 'in', 'mind', 'when', 'looking', 'at', 'the', 'price', 'this', 'is', 'going', 'to', 'last', 'you', 'quite', 'a', 'while', '.', 'another', 'co', '##sr', '##x', 'product', 'that', 'knocks', 'it', 'out', 'of', 'the', 'park', 'no', 'fr', '##ill', '##s', '/', 'misleading', 'marketing', 'messages', 'just', 'conscious', 'packaging', '&', 'ingredients', '.', 'try', 'the', 'rice', 'mask', 'for', 'night', 'as', 'well', 'however', 'try', '&', 'order', 'in', 'the', 'tube', 'it', 's', 'more', 'h', '##y', '##gie', '##nic', '.', 'i', 've', 'attached', 'a', 'photo', 'for', 'scale', '.']\n","\n","Token IDs:  [1045, 2196, 2245, 2008, 1045, 2052, 2424, 1996, 2157, 11052, 9496, 6290, 2005, 2026, 3096, 1012, 1045, 2031, 5257, 3096, 2007, 2312, 18499, 2229, 2008, 2131, 3282, 8126, 2039, 2065, 1045, 19046, 2068, 1998, 1045, 2310, 6915, 2005, 2070, 2051, 2000, 2424, 1037, 4031, 2008, 2052, 3073, 7182, 14098, 2302, 4911, 2033, 2041, 2030, 3810, 2026, 2227, 2046, 2019, 3514, 1011, 13554, 1012, 2023, 2003, 5621, 1037, 4151, 24665, 12502, 11052, 9496, 6290, 2005, 2033, 1998, 2038, 1037, 3835, 2227, 2514, 1998, 3019, 27849, 6518, 1012, 2097, 5791, 16360, 3126, 26300, 999, 1045, 1049, 1037, 2382, 2242, 4800, 1011, 5636, 14163, 2595, 2121, 1998, 1037, 3522, 10463, 2000, 4759, 5053, 1012, 2023, 2003, 2026, 2047, 9410, 1998, 2026, 3096, 2038, 2196, 2371, 2030, 2246, 6655, 12079, 999, 1015, 1007, 7221, 11733, 2522, 4550, 2009, 5717, 24501, 2121, 22879, 13153, 1016, 1007, 9253, 6914, 2665, 5572, 2613, 4840, 17952, 4550, 8043, 1017, 1007, 3335, 3270, 2051, 4329, 2034, 3949, 11305, 1006, 2851, 1007, 2030, 3335, 3270, 2051, 4329, 2305, 7192, 2047, 2671, 23713, 7140, 2571, 1006, 2305, 1007, 1018, 1007, 2522, 21338, 2595, 3514, 1011, 2489, 11087, 1011, 11052, 9496, 6774, 2843, 3258, 1006, 2851, 1998, 2412, 2060, 2305, 1007, 2030, 2522, 21338, 2595, 7209, 2053, 9496, 12227, 5785, 11585, 7308]\n","Token IDs:  [1045, 2318, 2478, 21677, 10207, 3723, 3688, 2044, 3352, 10206, 2197, 2095, 2007, 2026, 6233, 6450, 2137, 3688, 2008, 2790, 2000, 2468, 2625, 3466, 2004, 2051, 2253, 2006, 1012, 1045, 2179, 3243, 1037, 2261, 3688, 2008, 2147, 5186, 2092, 2011, 2522, 21338, 2595, 2023, 2108, 2028, 1997, 2068, 1012, 2085, 2004, 2524, 2004, 2009, 2089, 2022, 2000, 2424, 1996, 2157, 3096, 16302, 4531, 1996, 2157, 11052, 9496, 6290, 3849, 2000, 2022, 1037, 2196, 4566, 2645, 2053, 3043, 2054, 2060, 9727, 3688, 2089, 2022, 3564, 1999, 2115, 5723, 1012, 1045, 2001, 20221, 2000, 5309, 2023, 2012, 2034, 2349, 2000, 1996, 3976, 1004, 2649, 6518, 1012, 2174, 1996, 2946, 2029, 2003, 3492, 2312, 7126, 1996, 3976, 1004, 1996, 16421, 20066, 5437, 1006, 2025, 24980, 1007, 2008, 2001, 15366, 2033, 2941, 2150, 1037, 4121, 6781, 999, 1045, 2131, 9202, 14978, 2015, 1013, 25129, 2015, 2065, 1045, 2224, 2505, 2007, 1037, 6518, 2021, 1045, 2079, 3335, 2108, 2583, 2000, 2224, 2216, 3688, 2061, 2023, 2003, 1996, 3819, 5703, 2005, 2033, 1012, 2004, 2005, 4646, 1045, 2318, 2041, 2478, 1018, 1011, 1019, 15856, 2126, 2205, 2172, 999, 2625, 2003, 2062, 2009, 2064, 2599, 2000, 1037, 26484, 2298, 2174, 1996, 4031, 3475, 1056, 2941, 26484, 2074, 8279, 1012, 2085, 2008, 2009, 1055, 2621, 1045, 2224, 2028, 10216, 2006, 2026, 2690, 4344, 1004, 14548, 2009, 2090, 2026, 3093, 1998, 2059, 11112, 2009, 2046, 2026, 2227, 1012, 1045, 2310, 2036, 4162, 2009, 2007, 1037, 13773, 2063, 8248, 2007, 2307, 3463, 1012, 1996, 5835, 2153, 2003, 2025, 4714, 2061, 2562, 2008, 1999, 2568, 2043, 2559, 2012, 1996, 3976, 2023, 2003, 2183, 2000, 2197, 2017, 3243, 1037, 2096, 1012, 2178, 2522, 21338, 2595, 4031, 2008, 21145, 2009, 2041, 1997, 1996, 2380, 2053, 10424, 8591, 2015, 1013, 22369, 5821, 7696, 2074, 9715, 14793, 1004, 12760, 1012, 3046, 1996, 5785, 7308, 2005, 2305, 2004, 2092, 2174, 3046, 1004, 2344, 1999, 1996, 7270, 2009, 1055, 2062, 1044, 2100, 11239, 8713, 1012, 1045, 2310, 4987, 1037, 6302, 2005, 4094, 1012]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"rm4-Ma72ybJK"},"source":["### Step 5-2: Required Formatting\n","\n","- 각 문장의 처음과 끝에 special token 더하기\n","- 각 문장을 maximum length 만큼 자르고 padding token 채워주기\n","- 각 문장에서 padding token 과 실제 token 들 구분하기 위한 attention masking 적용\n","\n","#### Special Tokens\n","\n","- `[SEP]`: 모든 문장 뒤에 `[SEP]` 더하기\n","- `[CLS]`: 문장의 앞에 `[CLS]` 더하기\n","\n","<center><img src=\"https://github.com/passing2961/KEMC/blob/master/bert_clf.png?raw=true\" width=\"50%\" height=\"40%\" title=\"Bert4Clf\" alt=\"Bert4Clf\"></img></center>\n","\n","\n","#### Sentence Length & Attention Mask\n","\n","실제 데이터에 있는 문장들의 길이는 다양합니다. 이를 위해서 BERT 는 아래의 과정을 통해 해결합니다. \n","\n","- 모든 문장들은 하나의 고정된 길이인 max_len 를 지녀야하고, 이를 위해서 max_len 보다 긴 문장의 경우에는 잘라줍니다. 실제 문장의 길이가 max_len 보다 작은 경우에는 남은 부분을 padding token `[PAD]` 으로 채워줍니다.\n","  - `[PAD]`: BERT 사전에서 index 0 에 해당\n","- max_len 는 512 tokens 입니다.\n","\n","\n","<center><img src=\"https://github.com/passing2961/KEMC/blob/master/bert_pad.png?raw=true\" width=\"60%\" height=\"50%\" title=\"Bert4Pad\" alt=\"Bert4Pad\"></img></center>\n","\n","- **Attention mask**: 0과 1을 이용해 토큰이 `[PAD]` 인지 실제 값인지를 구분해주기 위한 binary tensor 값입니다. \n","BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 값입니다. 이 값은 0과 1 두 가지 값을 가지는데, 숫자 1은 해당 토큰은 실제 단어이므로 마스킹을 하지 않는다라는 의미이고, 숫자 0은 패딩 토큰이므로 마스킹을 한다는 의미입니다.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UDJ4ZBtizGK5"},"source":["#### Sentences to IDs\n","\n","`tokenizer.encode` 함수를 이용하여 모든 문장들에 대해 위의 과정들을 한꺼번에 처리합니다."]},{"cell_type":"code","metadata":{"id":"qArPy88KybJM","outputId":"939a40bb-403c-4667-ef96-f38da15e6827","executionInfo":{"status":"ok","timestamp":1665121548565,"user_tz":-540,"elapsed":10310,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Tokenize all of the sentences and map the tokens to their word IDs.\n","input_ids = []\n","\n","# For every sentence\n","for sent in train_data:\n","    # 'encode' will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the '[CLS]' token to the start.\n","    #   (3) Append the '[SEP]' token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   max_length : 문장의 최대길이\n","    #   encoded_sent : token IDs\n","    #---------------------------------------------------------------\n","    #      연습 (1)     tokenizer의 encode 함수를 호출해 주세요.  \n","    #---------------------------------------------------------------                \n","  \n","    encoded_sent = tokenizer.encode(sent, \n","                                    add_special_tokens=True,\n","                                    max_length = 64)\n","    \n","    # Add the encoded sentence to the list\n","    input_ids.append(encoded_sent)\n","\n","# Print train data[0]\n","print(\"Original: \", train_data[0])\n","print()\n","print(\"Token IDs: \", input_ids[0])\n","\n","# Print special tokens and tokenized sentence\n","print(\"\\n[CLS] token: {:}, ID: {:}\".format(tokenizer.cls_token, tokenizer.cls_token_id))\n","print(\"\\n[PAD] token: {:}, ID: {:}\".format(tokenizer.pad_token, tokenizer.pad_token_id))\n","print(\"\\n[SEP] token: {:}, ID: {:}\".format(tokenizer.sep_token, tokenizer.sep_token_id))\n","print(\"\\nTokenized: \", tokenizer.convert_ids_to_tokens(input_ids[0]))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Original:  I never thought that I would find the right moisturizer for my skin. I have combination skin with large pores that get gunked up if I neglect them and I ve struggled for some time to find a product that would provide sufficient moisture without breaking me out or turning my face into an oil-slick. This is truly a holy grail moisturizer for me and has a nice face feel and natural herbal scent. Will definitely repurchase!  I m a 30 something multi-ethnic muxer and a recent convert to korean beauty. This is my new routine and my skin has never felt or looked bettter!  1) Banila Co Clean It Zero Reservatrol 2) Neogen Green Tea Real Fresh Foam Cleanser 3) MISSHA Time Revolution First Treatment Essence (Morning) or MISSHA Time Revolution Night Repair New Science Ampoule (Night) 4) Cosrx Oil-Free Ultra-Moisturizing Lotion (Morning and ever OTHER night) or Cosrx Ultimate Nourishing Rice Overnight Mask\n","\n","Token IDs:  [101, 1045, 2196, 2245, 2008, 1045, 2052, 2424, 1996, 2157, 11052, 9496, 6290, 2005, 2026, 3096, 1012, 1045, 2031, 5257, 3096, 2007, 2312, 18499, 2229, 2008, 2131, 3282, 8126, 2039, 2065, 1045, 19046, 2068, 1998, 1045, 2310, 6915, 2005, 2070, 2051, 2000, 2424, 1037, 4031, 2008, 2052, 3073, 7182, 14098, 2302, 4911, 2033, 2041, 2030, 3810, 2026, 2227, 2046, 2019, 3514, 1011, 13554, 102]\n","\n","[CLS] token: [CLS], ID: 101\n","\n","[PAD] token: [PAD], ID: 0\n","\n","[SEP] token: [SEP], ID: 102\n","\n","Tokenized:  ['[CLS]', 'i', 'never', 'thought', 'that', 'i', 'would', 'find', 'the', 'right', 'moist', '##uri', '##zer', 'for', 'my', 'skin', '.', 'i', 'have', 'combination', 'skin', 'with', 'large', 'por', '##es', 'that', 'get', 'gun', '##ked', 'up', 'if', 'i', 'neglect', 'them', 'and', 'i', 've', 'struggled', 'for', 'some', 'time', 'to', 'find', 'a', 'product', 'that', 'would', 'provide', 'sufficient', 'moisture', 'without', 'breaking', 'me', 'out', 'or', 'turning', 'my', 'face', 'into', 'an', 'oil', '-', 'slick', '[SEP]']\n"]}]},{"cell_type":"markdown","metadata":{"id":"oKm4AQMFybJP"},"source":["#### Padding & Truncating\n","\n","`tf.keras.preprocessing.sequence.pad_sequences` 를 이용하여 **MAXLEN** 만큼 padding 과정을 진행합니다.\n"]},{"cell_type":"code","metadata":{"id":"qvi1_q33ybJQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665121898309,"user_tz":-540,"elapsed":506,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"e0d19173-6949-43ce-86f9-cf372765ecad"},"source":["print(\"Max length: \", max([len(each) for each in input_ids]))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Max length:  64\n"]}]},{"cell_type":"code","metadata":{"id":"cTelEtVNybJV","outputId":"55df993c-0b8b-4921-d705-ce3dbd6c984c","executionInfo":{"status":"ok","timestamp":1665121898841,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import tensorflow as tf\n","print(\"Tensorflow version: {}\".format(tf.__version__))\n","\n","MAXLEN = 64\n","# post-sequence truncation, post-sequence padding\n","# padding value 0, type of output sequences long\n","input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, #  input_ids 입력\n","                                                          maxlen=MAXLEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\") # post는 뒷단으로 하라 -> 길면 뒤에서 짤라라\n","\n","print(\"\\nPadding is done.\") # padding한 곳은 모두 0으로 채우기"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow version: 2.8.2\n","\n","Padding is done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZJnT5boqybJY"},"source":["#### Attention Masks\n","\n"]},{"cell_type":"code","metadata":{"id":"maKvU-KKybJZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665122101387,"user_tz":-540,"elapsed":1325,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"35b6c340-399e-426b-b7e6-20e4255e2457"},"source":["# Create attention masks\n","attention_masks = [] # 각 문장에 대한 attention mask 리스트를 저장\n","\n","# 토큰 시퀀스에서 패딩에 해당하는 부분은 0, 패딩이 아닌 부분은 1을 넣은 mask를 생성\n","# 패딩 부분은 모델 내에서 Attention을 수행하지 않아 학습속도를 향상\n","# For every sentence\n","for sent in input_ids: # input_ids는 토큰의 id 값이 학습 데이터의 개수만큼 있음 -> 10000여 개\n","    # Create the attention mask.\n","    #  - If a token ID is 0, then it's padding, set the mask to 0.\n","    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)\n","print(\"\\nAttention masking is done.\")"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Attention masking is done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"ISSN2XpEybJc"},"source":["#### Training & Validation Split\n","\n","`sklearn` 라이브러리에 `train_test_split` 함수를 이용하여 amazon review dataset 의 90% 는 training 으로 10% 는 validation 으로 나눠줍니다.\n","\n","- `random_state`: reproducibility 를 위함\n","\n","*Note: 아래 코드 블럭을 두 번 이상 실행시키면서 나오는 출력값을 확인해보세요.*\n","\n"]},{"cell_type":"code","metadata":{"id":"oTLBXYLTybJg","outputId":"8236d051-7421-4a68-fc7a-a67e1eff9ac1","executionInfo":{"status":"ok","timestamp":1665122446046,"user_tz":-540,"elapsed":516,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Use train_test_split to split our data into train and validation sets for training\n","from sklearn.model_selection import train_test_split\n","\n","# train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n","#     arrays : 분할시킬 데이터\n","#     test_size : 테스트 데이터셋의 비율 (default = 0.25)\n","#     random_state : 데이터 셔플 시 seed value. 호출할 때마다 동일한 학습 데이터, 테스트 데이터 셋을 생성하기 위해 설정\n","#     shuffle : 셔플 여부 (default = True)\n","#     stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때\n","#     stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지\n","# 반환값 : (학습 데이터, 테스트 데이터, 학습데이터 label, 테스트데이터 label)\n","# test_size=0.1로 설정, Use 90% for training and 10% for validation\n","train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, train_label, random_state=2018, test_size=0.1)\n","\n","# Do the same for the masks.\n","train_masks, valid_masks, _, _ = train_test_split(attention_masks, train_label, random_state=2018, test_size=0.1)\n","\n","# print train_inputs, valid_inputs\n","# 첫번째 학습데이터, 첫번째 attention mask를 출력해서 확인\n","print(train_inputs[:1])\n","print(train_masks[:1])"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  101  4149  2023  2043  2559  2005  2227  3688  3647  1999 10032  1012\n","   1045  2293  2009  1998  2097  3613  2000  2224  2035  1996  2051  2085\n","   1012   102     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0]]\n","[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"p_jmnF2IybJj"},"source":["#### Converting to PyTorch Data Types"]},{"cell_type":"code","metadata":{"id":"5hFzBxvyybJk","executionInfo":{"status":"ok","timestamp":1665122608719,"user_tz":-540,"elapsed":704,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["# Convert all inputs and labels into torch tensors, the required data type for our model.\n","# torch.tensor(data) -> tensor를 생성\n","train_inputs = torch.tensor(train_inputs)\n","valid_inputs = torch.tensor(valid_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","valid_labels = torch.tensor(valid_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","valid_masks = torch.tensor(valid_masks)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4J9aJtLU96be"},"source":["PyTorch 의 `DataLoader` class 를 이용하여 amazon review dataset 에 대한 `iterator` 를 생성합니다.\n","\n","- `TensorDataset`: tensor 를 입력받아 dataset 형태로 변환해주는 함수\n","- `RandomSampler`: 입력 dataset 에 element 들의 index 를 무작위로 샘플링하는 함수\n","- `SequentialSampler`: 입력 dataset 에 element 들의 index 를 순차적으로 샘플링하는 함수\n","- `DataLoader`: dataset 과 sampler 를 이용하여 주어진 dataset 에 대하여 배치사이즈만큼 데이터를 반환해주는 함수\n"]},{"cell_type":"code","metadata":{"id":"xhbYgxdvybJq","executionInfo":{"status":"ok","timestamp":1665122609193,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["# torch.utils.data : 파이토치의 데이터 로딩 유틸리티. 데이터셋, 데이터로더, 샘플러 등을 제공함\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it here.\n","# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n","\n","batch_size = 32\n","\n","#---------------------------------------------------------------\n","#      연습 (2) train_data, valid_data의 데이터 로더를 생성해 주세요.\n","#---------------------------------------------------------------   \n","# Create the DataLoader for our training set.\n","# torch.utils.data.TensorDataset(*tensors)\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n","valid_sampler = SequentialSampler(valid_data)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JR7FaNXOybJu"},"source":["## Step 6: Train our classification model\n","\n","Pre-train 된 BERT 모델을 불러와서 그 위에 single linear layer 하나를 올립니다. 그리고 전체 모델을 fine-tuning 을 진행합니다."]},{"cell_type":"markdown","metadata":{"id":"dxNAGHYwybJv"},"source":["### Step 6-1: BertForSequenceClassification\n","\n","`huggingface` 라이브러리에서 제공하는 `BertForSequenceClassification` 모델을 사용하면 우리가 원하는 모델을 얻을 수 있습니다. 즉, pre-train BERT 모델 위에 single linear layer 하나가 더해져있는 모델을 불러옵니다. 따라서, 해당 모델을 우리가 원하는 task 에 맞게 end-to-end 방식으로 fine-tuning 을 진행할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"V7JYZMxEybJx","outputId":"e7a6a2ac-e954-45d7-99fd-7ba24ea8f592","executionInfo":{"status":"ok","timestamp":1665122908660,"user_tz":-540,"elapsed":17128,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["58f616575f294c20b1652f6bff37b6b9","a146ad3ab92e426ba396da1c48850211","ac4acd3bd8024237969f39377bce9015","033e3c9111684ad1989d45dd5e6e2635","7ab5f36206174d7e8577d45a370302bd","46b5b58ed9524bf48f2d17d2c0715c2d","42e1e0ca254d4df7a55aadf8db0c56cc","19f8919d27ea468f90444aa3cff409b4","466e30cc190649928deef9dab1f77d2b","e0ab6b57d9a5430db93cd55236708f50","7a18c7b40bd54ad89463fefed9824d1f"]}},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassfication, the pretrained BERT model \n","# with a single linear classification layer on top.\n","# BERT 모델의 네트워크 형태를 출력\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n","                                                      num_labels = 2, # The number of output labels 2 for binary classification\n","                                                                      # You can increase this for multi-class tasks\n","                                                      output_attentions = False, # whether the model returns attentions weight (correponding to multi-head self attentions)\n","                                                      output_hidden_states = False) # whether the model returns all hidden states\n","\n","# Tell PyTorch to run this model on the GPU\n","# (model의 모든 parameter를 GPU에 loading)\n","model.cuda()"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f616575f294c20b1652f6bff37b6b9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"WX9bftmNybJ0","outputId":"1d90b499-3ead-4ab5-da94-2eaea16df6a8","executionInfo":{"status":"ok","timestamp":1665124627720,"user_tz":-540,"elapsed":490,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print(\"The BERT model has {:} different named parameters.\\n\".format(len(params)))\n","print(\"=== Embedding Layer ===\\n\")\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print(\"\\n==== First Transformer ====\\n\")\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print(\"\\n==== Output Layer====\\n\")\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["The BERT model has 201 different named parameters.\n","\n","=== Embedding Layer ===\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"acXpmSC7ybJ5"},"source":["### Step 6-2: Optimizer & Learning Rate Scheduler\n","\n","- `optimizer`: 모델의 loss function 을 최소화되게 하는 모델의 파라미터들을 찾는 알고리즘\n","- `learning rate scheduler`: learning rate 를 조절해주는 방법\n","  - `learning rate warmup`: 초기에 learning rate 를 0 으로 설정하고 이를 일정 기간동안 heurisitc 하게 키워주는 방식 (*초기값이 너무 크면 학습이 불안정*)\n","\n","- `learning rate warmup` 관련 논문: https://arxiv.org/abs/1812.01187"]},{"cell_type":"code","metadata":{"id":"GN8StKalybJ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665124630489,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"6d616694-964e-4f7c-f2cb-776ec8ec1ddf"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n","# I believe the 'W' stands for \"Weight Decay fix\"\n","# Weight Decay: weight들의 값이 증가하는 것을 제한함으로써 모델의 복잡도를 감소시켜 오버피팅을 방지하는 기법 \n","optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"dvTMmRTvybJ8","outputId":"8908a515-676c-4973-8399-c7c0fd957603","executionInfo":{"status":"ok","timestamp":1665124631054,"user_tz":-540,"elapsed":3,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (we recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","print(\"number of batches:\", len(train_dataloader))\n","total_steps = len(train_dataloader) * epochs\n","\n","print(\"total_steps:\", total_steps)\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["number of batches: 302\n","total_steps: 1208\n"]}]},{"cell_type":"markdown","metadata":{"id":"MdPUjq3GybKA"},"source":["### Step 6-3: Training Loop\n","\n","\n","#### **본 실습에서 학습 과정**\n","\n","- `DataLoader` 를 통해 data 와 label 배치사이즈 단위로 가져오기\n","- Forward pass: data 와 label 을 `BertForSequenceClassification` 모델에 입력으로 주기\n","- Backward pass:\n","  - minibatch gradient 계산 (backpropagation)\n","  - 모델의 파라미터들 업데이트 (optimization)\n","  \n","\n"]},{"cell_type":"code","metadata":{"id":"Pigy1CqrybKA","executionInfo":{"status":"ok","timestamp":1665124636991,"user_tz":-540,"elapsed":840,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","# numpy argmax : 해당 차원(axis)의 값 중에서 가장 큰 값의 인덱스를 반환\n","# flatten() : 다차원 배열을 1차원으로 변환\n","# sum() : 배열 내 전체 값들의 합\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXBpr-HAybKD","executionInfo":{"status":"ok","timestamp":1665124636993,"user_tz":-540,"elapsed":15,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Take a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round(elapsed))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0QRDrYcybKG","executionInfo":{"status":"ok","timestamp":1665124636994,"user_tz":-540,"elapsed":15,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["import random\n","\n","# Set the seed value all over the place to make this reproducible\n","def set_seed(seed_val):\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOCgIIrpybKI","outputId":"f2bf6132-3855-4fa0-d5be-da2dd5b119c8","executionInfo":{"status":"ok","timestamp":1665125065446,"user_tz":-540,"elapsed":428464,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","seed_val = 42\n","set_seed(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them\n","loss_values = []\n","\n","# For each epoch\n","for epoch in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n","    print('Training...')\n","    \n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","    \n","    # Reset the total loss for this epoch.\n","    total_loss = 0.\n","    \n","    # Put the model into training mode.\n","    # Don't be mislead -- the call to 'train' just changes the \"mode\", it doesn't \"perform\" the training.\n","    # 'dropout' and 'bachnorm' layers behave differently during training vs test\n","    model.train()\n","    \n","    # For each batch of training data\n","    for step, batch in enumerate(train_dataloader):\n","        \n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress\n","            print(\"Batch {:>5,} of {:>5,}. Elapsed: {:}.\".format(step, len(train_dataloader), elapsed))\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        #---------------------------------------------------------------\n","        #      연습 (3) 아래 라인에 그래디언트를 0으로 초기화하는 함수를 호출해 주세요.\n","        #--------------------------------------------------------------- \n","        model.zero_grad()\n","        \n","        # Perform a forward pass (evaluate the model on this training batch).\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","        \n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value from the tensor.\n","        total_loss += loss.item()\n","        \n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        \n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        \n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.        \n","        #---------------------------------------------------------------\n","        #      연습 (4) 아래 라인에 파라미터를 업데이트하는 함수를 호출해 주세요.\n","        #--------------------------------------------------------------- \n","        optimizer.step()\n","        \n","        # Update the learning rate.\n","        scheduler.step()\n","    \n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","    \n","    print(\"\")\n","    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","    \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on our validation set.\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # Put the model in evaluation mode -- the dropout layers behave differently during evaluation\n","    model.eval()\n","    \n","    # Tracking variables\n","    eval_loss, eval_acc = 0., 0.\n","    \n","    # Evaluate data for one epoch\n","    for valid_step, batch in enumerate(valid_dataloader):\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","        with torch.no_grad():\n","            \n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have not provided labels.\n","            #---------------------------------------------------------------\n","            #      연습 (5) 아래 함수에서 파라미터를 채워주세요\n","            #--------------------------------------------------------------- \n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","            \n","        # Get the \"logits\" output by the model.\n","        # The \"logits\" are the output values prior to applying an activation function like the softmax\n","        # outpus 타입: class transformers.modeling_outputs.SequenceClassifierOutput\n","        # (https://huggingface.co/docs/transformers/v4.22.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)\n","        # our model will return (outputs.loss(optional)=None, outputs.logits)\n","        \n","        logits = outputs[0]\n","        \n","        # Move logits and labels to CPU (Module을 통해 나온 tensor을 후처리에 사용하거나, 계산된 loss를 로깅 등)\n","        # detach() : 파이토치는 tensor에서 이루어진 모든 연산을 추적해서 graph에 기록해두는데 이 연산 기록으로부터 \n","        # 그래디언트가 계산되고 역전파가 이루어지게 된다. detach()는 이 연산 기록으로부터 분리한 tensor을 반환하는 method     \n","        # cpu() : GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사하는 method   \n","        # numpy() : tensor를 numpy로 변환하여 반환\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_acc = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_acc += tmp_eval_acc        \n","    \n","    # Report the final accuracy for this validation run.\n","    print(\"Accuracy: {0:.2f}\".format(eval_acc / (valid_step + 1)))\n","    print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")        "],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","Batch    40 of   302. Elapsed: 0:00:16.\n","Batch    80 of   302. Elapsed: 0:00:29.\n","Batch   120 of   302. Elapsed: 0:00:42.\n","Batch   160 of   302. Elapsed: 0:00:56.\n","Batch   200 of   302. Elapsed: 0:01:09.\n","Batch   240 of   302. Elapsed: 0:01:23.\n","Batch   280 of   302. Elapsed: 0:01:37.\n","\n","Average training loss: 0.16\n","Training epoch took: 0:01:44\n","\n","Running Validation...\n","Accuracy: 0.97\n","Validation took: 0:00:04\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","Batch    40 of   302. Elapsed: 0:00:14.\n","Batch    80 of   302. Elapsed: 0:00:27.\n","Batch   120 of   302. Elapsed: 0:00:41.\n","Batch   160 of   302. Elapsed: 0:00:54.\n","Batch   200 of   302. Elapsed: 0:01:08.\n","Batch   240 of   302. Elapsed: 0:01:22.\n","Batch   280 of   302. Elapsed: 0:01:36.\n","\n","Average training loss: 0.06\n","Training epoch took: 0:01:43\n","\n","Running Validation...\n","Accuracy: 0.97\n","Validation took: 0:00:04\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","Batch    40 of   302. Elapsed: 0:00:14.\n","Batch    80 of   302. Elapsed: 0:00:27.\n","Batch   120 of   302. Elapsed: 0:00:41.\n","Batch   160 of   302. Elapsed: 0:00:55.\n","Batch   200 of   302. Elapsed: 0:01:08.\n","Batch   240 of   302. Elapsed: 0:01:22.\n","Batch   280 of   302. Elapsed: 0:01:36.\n","\n","Average training loss: 0.03\n","Training epoch took: 0:01:43\n","\n","Running Validation...\n","Accuracy: 0.97\n","Validation took: 0:00:04\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","Batch    40 of   302. Elapsed: 0:00:14.\n","Batch    80 of   302. Elapsed: 0:00:27.\n","Batch   120 of   302. Elapsed: 0:00:41.\n","Batch   160 of   302. Elapsed: 0:00:55.\n","Batch   200 of   302. Elapsed: 0:01:08.\n","Batch   240 of   302. Elapsed: 0:01:22.\n","Batch   280 of   302. Elapsed: 0:01:36.\n","\n","Average training loss: 0.01\n","Training epoch took: 0:01:43\n","\n","Running Validation...\n","Accuracy: 0.97\n","Validation took: 0:00:04\n","\n","Training complete!\n"]}]},{"cell_type":"code","metadata":{"id":"7HEWjeywybKL","executionInfo":{"status":"ok","timestamp":1665125137494,"user_tz":-540,"elapsed":835,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/","height":427},"outputId":"b8ff2ceb-952a-4619-8422-23d4cac1609d"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"qVQTc7Yd5wR8"},"source":["## Step 7: Performance on Test set\n","\n","- `sklearn` 라이브러리의 `classification_report` 함수 이용하여 **accuracy, precision, recall, f1 score** 평가 지표를 이용하여 test dataset 에 대해 성능 확인\n","  - dataset 의 label 이 균형적인 경우 (balanced data): **accuracy**\n","  - dataset 의 label 이 불균형적인 경우 (imbalanced data): **f1 score**"]},{"cell_type":"markdown","metadata":{"id":"eE5hAam9WjWf"},"source":["### Step 7-1: Load Amazon Review Test Dataset\n"]},{"cell_type":"code","metadata":{"id":"jQkChrxk5YLN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665125143414,"user_tz":-540,"elapsed":1710,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"outputId":"15462174-58ac-4341-bd73-12d70dae1502"},"source":["# pickle 모듈로 테스트 파일을 load : bert_balanced_data, bert_balanced_label\n","with open(test_data, 'rb') as f:\n","    test_data = pc.load(f)\n","with open(test_label, 'rb') as f:\n","    test_label = pc.load(f)\n","\n","\n","print(\"Size of test data: {}\".format(len(test_data)))\n","print(\"Size of test label: {}\".format(len(test_label)))\n"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of test data: 174\n","Size of test label: 174\n"]}]},{"cell_type":"markdown","metadata":{"id":"y8GJHfrEW39t"},"source":["### Step 7-2: Tokenization & Input Formatting\n","\n","본 단계에서는 fine-tuning layer 까지 학습한 BERT 모델의 입력 format 에 맞게 amazon review test dataset 을 변환해줍니다. (*위에서 진행한 방법과 동일*)"]},{"cell_type":"code","metadata":{"id":"PIg14zYC50F1","executionInfo":{"status":"ok","timestamp":1665125143415,"user_tz":-540,"elapsed":4,"user":{"displayName":"정연수","userId":"02873126808620457795"}}},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in test_data:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAXLEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks array\n","attention_masks = []\n","\n","# For every sentence\n","for sent in input_ids:\n","    # Create the attention mask.\n","    #  - If a token ID is 0, then it's padding, set the mask to 0.\n","    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)\n","\n","# Convert to tensors\n","# input_ids : 테스트 문장에 포함된 단어 ids\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(test_label)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","prediction_sampler = SequentialSampler(prediction_data) \n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-dlllY7XkJ-"},"source":["### Step 7-3: Evaluate on Test Set"]},{"cell_type":"code","metadata":{"id":"9PK69aZG6kWt","executionInfo":{"status":"ok","timestamp":1665125145511,"user_tz":-540,"elapsed":1463,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2df85bc5-009b-487e-8795-7462b4f4f1a3"},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions, true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader (batch에서 데이터 추출)\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # 모델 불러주기\n","\n","  logits = outputs[0] # logit이 들어있는 상태이므로 output[0]을 가져오면 logit을 받아올 수 있음, 4가지 정도의 리턴 값이 있음\n","\n","  # Move logits and labels to CPU\n","  # logist, labels 텐서를 CPU로 이동하여 정확도 계산\n","  # detach() : 텐서에서 이루어진 연산 기록으로부터 분리한 텐서를 반환\n","  # cpu() : GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사\n","  # numpy() : tensor를 numpy로 변환하여 반환\n"," \n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","\n","  # Store predictions and true labels\n","  # predictions : 예측값 array\n","  # true_labels : 정답값 array\n","  # logits array의 값들 중 가장 큰 값의 index를 반환\n","  # when axis = 1, argmax identifies the maximum value for every row. \n","  predictions.extend(np.argmax(logits, axis=1).flatten()) # [0, 1, 1, 0, ...]\n","  true_labels.extend(label_ids.flatten()) # [0, 1, 0, 0, ...]\n","\n","\n","print('DONE.')"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 174 test sentences...\n","DONE.\n"]}]},{"cell_type":"code","metadata":{"id":"Q3o8SfUnV--K","executionInfo":{"status":"ok","timestamp":1665125146027,"user_tz":-540,"elapsed":4,"user":{"displayName":"정연수","userId":"02873126808620457795"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"483e0230-7823-483b-bf0c-3e2a75134048"},"source":["# accuracy, precision, recall, f1 score 성능 확인\n","from sklearn.metrics import classification_report\n","    \n","target_names = ['negative', 'positive']\n","\n","# sklearn.metrics.classification_report(y_true, y_pred, digits, target_names)\n","# support is the number of actual occurrences of the class in the specified dataset.\n","print(classification_report(true_labels, predictions, digits=4, target_names=target_names))"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    negative     0.9722    0.8046    0.8805        87\n","    positive     0.8333    0.9770    0.8995        87\n","\n","    accuracy                         0.8908       174\n","   macro avg     0.9028    0.8908    0.8900       174\n","weighted avg     0.9028    0.8908    0.8900       174\n","\n"]}]},{"cell_type":"markdown","source":["[알림] BERT 코드에서 STEP 6-3 outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) -> token_type_ids 는 segment id 리스트입니다 (첫번째 문장 0, 두번째 문장 1)  \n","\n","그런데 우리 실습에서는 single sentence 이므로 None으로 부여합니다"],"metadata":{"id":"ffJNeyJAX0im"}},{"cell_type":"code","metadata":{"id":"5qH_FBEWslsB"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"fSXDIKEbX3uO"}}]}