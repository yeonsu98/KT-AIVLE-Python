{"cells":[{"cell_type":"markdown","metadata":{"id":"4s7uNic-sKFN"},"source":["# Seq2Seq 모델을 사용한 챗봇 구현 튜토리얼\n","\n","코드&설명 제공 (Thanks to)\n","- original code by 이영준 (2020 KEPCO 문제해결형 인공지능기술개발교육 TA)\n","- 토크나이저 변경 by 조상현 (부산대학교 AI대학원 자연어처리 TA)\n","\n","\n","본 실습에서는 sequence-to-sequence (seq2seq) 모델을 이용하여 생성 챗봇을 구현한다.\n","\n","학습데이터로 일상대화 데이터 쌍 (약 12,000건)을 사용한다.\n","\n","**생성 챗봇 대화 예시**: \n","<pre>\n","<code>\n","문장을 입력하세요: 안녕\n","Bot: 안녕하시어요.\n","문장을 입력하세요: 갈까 말까?\n","Bot: 가시어요.\n","문장을 입력하세요: 12시 땡!\n","Bot: 하루가 또 가네요.\n","</code>\n","</pre>\n","\n","## 원본 코드, 데이터 출처\n","[chatbot 튜토리얼]: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#\n","[한국어 대화 데이터]: https://github.com/songys/Chatbot_data\n","\n","- 본 실습 코드는 PyTorch 에서 제공하는 [chatbot 튜토리얼]을 참고하였습니다.\n","- seq2seq 모델의 학습을 위해 [한국어 대화 데이터]를 사용하였습니다."]},{"cell_type":"markdown","metadata":{"id":"BUzVAkY_sKFP"},"source":["## Step 0: Connect to Google drive\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22395,"status":"ok","timestamp":1664693813534,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"A2k5srlmsKFP","outputId":"fe58d491-2a31-484a-a065-fafeb62a543a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"TL1C8SdmsKFT"},"source":["## Step 1: Import module"]},{"cell_type":"markdown","source":["re : 정규표현식 사용을 위한 내장 모듈\n","\n","collections : 파이썬의 자료형(list, tuple, dict)들에게 확장된 기능을 제공하는 내장 모듈. vocab 구축 시 사용 \n","\n","os : 기본 내장 모듈로서 경로생성 함수 제공"],"metadata":{"id":"kzAvpRhVSBQ7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2350,"status":"ok","timestamp":1664693819138,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"UwGXnXUB92BB","outputId":"6678480d-f29b-4c37-8e6a-598b1f1e78c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch Version:  1.12.1+cu113\n","There are 1 GPU(s) available.\n","We will use the GPU: A100-SXM4-40GB\n"]}],"source":["# 라이브러리 호출\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import re\n","import random\n","import os\n","import csv\n","import pickle as pc\n","import collections\n","import numpy as np\n","\n","# reproducibility (재실행시마다 동일한 결과를 얻기 위해서)\n","SEED = 470\n","random.seed(SEED)                 # python random 라이브러리의 random seed를 고정\n","np.random.seed(SEED)              # numpy에서 난수 생성시 random seed를 고정\n","torch.manual_seed(SEED)           # CPU 연산시 pytorch에서 random seed를 고정\n","torch.cuda.manual_seed(SEED)      # GPU 연산시 pytorch에서 random seed를 고정\n","torch.cuda.manual_seed_all(SEED)  # 멀티 GPU 연산 시 random seed를 고정\n","\n","# torch 버전 확인\n","print(\"Pytorch Version: \", torch.__version__)\n","\n","# GPU 사용 가능 여부 확인\n","if torch.cuda.is_available():\n","    # PyTorch GPU 사용 설정\n","    device = torch.device(\"cuda\")\n","    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n","    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n","else:\n","    print(\"No GPU available, using the CPU instead.\")\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"MtoL6_QfsKFY"},"source":["## Step 2: Configure the experiments\n","\n","- 모델의 실험(학습 및 평가)을 필요한 파라미터 및 인자 설정\n","    - Hyperparameter: hidden size, vocabulary size, max length, dropout rate 등\n","    - Argument: file directory 등"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFnYo08gsKFZ"},"outputs":[],"source":["# 데이터, 모델 위치 (직접 설정이 필요함)\n","data_dir = '/content/gdrive/My Drive/Colab Notebooks/aivle/data/songys/ChatbotData.csv'\n","dirpath = '/content/gdrive/My Drive/Colab Notebooks/aivle/model/'\n","\n","if not os.path.exists(dirpath):   \n","    os.makedirs(dirpath)    # dirpath가 없으면 해당 디렉토리를 생성해줌\n","\n","# word2idx : binary file (단어 -> index로 매핑)    \n","WORD_DICT_DIR = '/content/gdrive/My Drive/Colab Notebooks/aivle/data/songys/word2idx'\n","\n","THRESHOLD = 40000 #dictionary에서 토큰 갯수의 한계값 설정\n","MAX_LEN = 25  #문장의 최대 길이\n","\n","attn_model = 'dot'  # 어텐션 기법 : dot 프로덕트\n","\n","#실행이 오래걸리지 않도록 인코더와 디코더의 hidden_size, layer_number를 작게 설정\n","enc_hidden_size = 200\n","dec_hidden_size = 400  # 인코더가 양방향이므로\n","encoder_n_layers = 1  \n","decoder_n_layers = 1\n","\n","dropout = 0.1 # 랜덤하게 정한 Weight의 10퍼센트는 사용하지 않음 -> 오버피팅 방지\n","batch_size = 32 #연산 한 번에 들어가는 학습 데이터의 갯수 32개\n","max_epochs = 10 #전체 학습데이터 셋이 신경망을 통과한 횟수\n","\n","learning_rate = 0.001 #학습률 : 옵티마이저에서 파라미터값을 변경하는 비율\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LEwn1vkRsKFc"},"source":["## Step 3: Data preparation\n","\n","모델의 학습을 위해 데이터를 준비하는 과정이고 다음과 같다.\n","    \n","   - Load data\n","   - Tokenization\n","   - Build vocab"]},{"cell_type":"markdown","metadata":{"id":"QVhCDjx6X9zP"},"source":["### Step 3-1: Load data\n","\n","- **\"ChatbotData.csv\"** 파일 load\n","- **`[utterance, response]`** pair 의 형태로 재구성"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":998,"status":"ok","timestamp":1664693828467,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"c-MNHEDSsKFd","outputId":"e6c1d976-e98f-4089-a1b1-2c15eb75c572"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['12시 땡!', '하루가 또 가네요.'], ['1지망 학교 떨어졌어', '위로해 드립니다.'], ['3박4일 놀러가고 싶다', '여행은 언제나 좋죠.']]\n"]}],"source":["# pair data load\n","pair_data = list() # 비어 있는 list 생성\n","\n","f = open(data_dir, 'r', encoding='utf-8')\n","reader = csv.reader(f)      # reader에서 콤마(,)를 구분자로 질문, 대답 부분을 읽어옴\n","for idx, line in enumerate(reader):\n","    if idx == 0:    # 첫번째 라인은 skip (파일 포맷에 대한 코멘트 라인)\n","        continue\n","        \n","    pair_data.append([line[0], line[1]])\n","f.close()\n","\n","# pair data 확인\n","print(pair_data[:3])  # pair_data는 리스트의 리스트 타입"]},{"cell_type":"markdown","metadata":{"id":"s-mz5L-bsKFf"},"source":["### Step 3-2: Tokenization\n","\n","한국어 Word Piece 방식 토크나이저를 사용합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8653,"status":"ok","timestamp":1664693839711,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"xy6t4doLE9AF","outputId":"641065ae-c582-43e1-c8f7-85515b56b7b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 82.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 71.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["533032f1b5f24db6ace9455a6c90ab30","7688817835fc45e4b4359fb084690a55","52ee3528ab6e4d4e915cb23ee6aae2ab","5dac1d88aad2481ca4adbba8d4222fe1","fd8c2f752e734df2ad37a38eceef14d4","3cf9535bde93488b80257f330524d49c","b89f33b3d2804de29c7b32dcad5ac1ed","6803f37fa0e94202b860f4ab1863e388","881af838ff7941419d4ff51a63304eb2","74e9dc27041a432da4677d0c46783058","02a656ded174429ea65d684754331709","d2c6fd045d674ebaa63de32ab8a8ebe3","7df7c388ff754ffe8ee02bd45ead6b33","ceed8a860d4f49ba95e6e925ca4ff525","b1d465d2e0c34b4ba7cc8c000ed2b1ff","616c2c157e8545028dbb7ae548237220","6103b5b0fc49455581a46af9d490e2ca","bd72994f856949d3847187eb37bb48cb","d712fa5fb50b4781a88c543c8a30ef87","74f494f65d7a4c0da5cee0d76448be5e","cba3fad142394ce89113de5348920709","c94559938a2e4edf95bd26b46f9eb1ce","8395ae1616df489ba33f6c219956ad24","ab502e97b25a46cb96184522098031c8","d4f273b2a37d47a0ac890c2c2d709830","b1e859c705fb44f9a4bae06d99746cce","167066d9340e42e59aaa7c6ae7a78c54","cffe96143d3e46e399c9038a84b4c5e2","ee13ced332ad4bb0a1bb5f2fc082c5f6","078c6392a0a648918be6eb4659526b34","b19a723ed0c84695beeae8977bb5b921","84f243f21ad44b2797f8469d92adc668","97acd49bfed14cb28c5253eaa14b8525","461c89689a644734aaa755a25da3e495","06f7a6102787462b9d6dfe812bd56708","e1cf94f1f3ac4d2fbe6f43cbb82af661","d1e14b7196f2474681dc30eb6c0a514c","80e077170aac4703adaab7e9302ba2fd","cb8f306cb0cd4a9189ca2d42df19186b","1c3189e110e44462b31049e039c50ea0","e669c2773b2547a5a4fefdd2485f6cf6","35d91b3049c9408397cb45dc85e36d6f","bd734ea368f94372b00b2fd4b92f6d13","dba05c32c44d4417a5f9859c80abbb3a"]},"executionInfo":{"elapsed":1155,"status":"ok","timestamp":1664693844032,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"Bxqz32VEFB3c","outputId":"6bc40fe9-1033-4979-e862-55cf4d5775d0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/373 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533032f1b5f24db6ace9455a6c90ab30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/241k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c6fd045d674ebaa63de32ab8a8ebe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/492k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8395ae1616df489ba33f6c219956ad24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/169 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461c89689a644734aaa755a25da3e495"}},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2125,"status":"ok","timestamp":1664693870433,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"7zH_CbPM34fw","outputId":"6352ec54-ba4f-4c22-da13-294902114341"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total size of data is 11823\n","\n","Example:\n","[[['12', '##시', '땡', '!'], ['하루', '##가', '또', '가네', '##요', '.']]]\n"]}],"source":["# 형태소 분석된 [질문, 응답] 데이터셋 구축\n","total_data = list()   # 빈 리스트를 생성\n","for each in pair_data:\n","    utter = tokenizer.tokenize(each[0])\n","    resp = tokenizer.tokenize(each[1])\n","    total_data.append([utter, resp])\n","    \n","# 데이터 사이즈 및 실제 결과 확인\n","print(\"Total size of data is\", len(total_data))\n","print(\"\\nExample:\")\n","print(total_data[:1])"]},{"cell_type":"markdown","metadata":{"id":"wdnV9BVfsKFn"},"source":["### Step 3-3: Data split & shuffling \n","\n","[링크]: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","\n","학습 및 평가를 위해 원본 데이터에서 **90%** 는 학습 데이터로 사용하고, **10%** 는 평가 데이터로 사용합니다. 이를 위해 `sklearn` 라이브러리에 `train_test_split` 함수를 사용하고, 함수의 argument 정보들은 해당 [링크]에서 확인할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":725,"status":"ok","timestamp":1664693873829,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"3zs1uXSSsKFo","outputId":"6e5b7061-cd31-432d-aabf-fdbe2e519de4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Test size is 10640/1183\n","\n","Example:\n","[[['나', '##한', '##테', '질리', '##면', '어쩌', '##지', '걱정', '##돼'], ['당신', '##의', '겉모습', '##이', '아닌', '진정한', '내면', '##의', '모습', '##을', '보여', '##주', '##세요', '.']], [['새로운', '일', '벌려', '##도', '될까'], ['도전', '##해', '봐도', '좋', '##을', '거', '같', '##아', '##요', '.']]]\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","train, test = train_test_split(total_data, test_size=0.1, random_state=42, shuffle=True)\n","\n","# 학습 및 평가 데이터 크기 확인\n","print(\"Train/Test size is {}/{}\".format(len(train), len(test)))\n","print(\"\\nExample:\")\n","print(train[:2])"]},{"cell_type":"markdown","metadata":{"id":"IfGKoqU4sKF2"},"source":["### Step 3-4: Create Word Dictionary\n","\n","실제 자연어로 이루어진 단어들을 기계가 이해할 수 있는 index 값으로 맵핑해주는 dictionary를 구축합니다. 추가로, 응답을 생성하는 단계를 위해 index가 단어로 맵핑되는 dictionary도 구축합니다.\n","\n","- `build_dict`: 학습 데이터의 토큰들을 빈도수 단위로 내림차순 정렬하여, 위에서부터 **threshold** 기준으로 dictionary 갯수/사이즈를 지정한다.\n","    - special tokens\n","        - **pad**: GPU를 이용하여 모델을 학습시키기 위해서는 batch 내에 있는 모든 문장들이 동일한 길이를 가져야한다. 이를 위해, maximum 길이를 지정하고 남은 부분은 padding token 을 채워줍니다. 주의할 점은, 학습시 loss 를 구할 때 padding 에 해당되는 부분은 반영시키지 않아야 합니다.\n","        - **unk**: word dictionary 에 없는 단어(token)가 등장하면 unknown token 을 채워줍니다.\n","        - **sos**: 디코더에서 문장의 시작을 알리는 토큰입니다.\n","        - **eos**: 디코더에서 문장의 끝을 알리는 토큰입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZP8YkNkJsKF3"},"outputs":[],"source":["EOS_token = 3\n","SOS_token = 2\n","UNK_token = 1\n","PAD_token = 0\n","\n","def build_dict(data, threshold=40000):\n","    \n","    if not os.path.exists(WORD_DICT_DIR) or True:\n","        \"\"\"\n","        Build word dictionary\n","        \"\"\"\n","        \n","        vocab = list()\n","        for doc in data:\n","            for word in doc[0]:\n","                vocab.append(word)\n","            for word in doc[1]:\n","                vocab.append(word)\n","        # 단어의 빈도수 순서로 vocab을 sorting한다. vocab의 최대 크기는 threshold\n","        counter = collections.Counter(vocab).most_common(threshold)\n","        \n","        word2idx = dict()\n","        word2idx['<pad>'] = PAD_token #0\n","        word2idx['<unk>'] = UNK_token #1\n","        word2idx['<sos>'] = SOS_token #2\n","        word2idx['<eos>'] = EOS_token #3\n","        \n","        # 고빈도 단어부터 ID를 부여함\n","        for word, _ in counter:\n","            word2idx[word] = len(word2idx)\n","        \n","        with open(WORD_DICT_DIR, 'wb') as f:\n","            pc.dump(word2idx, f)\n","    else:\n","        \"\"\"\n","        Load word dictionary which was built before\n","        \"\"\"\n","        with open(WORD_DICT_DIR, 'rb') as f:\n","            word2idx = pc.load(f)\n","    \n","    print(\"Load word dictionary\")\n","    return word2idx     # (단어 : index) 포맷의 사전을 반환"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":844,"status":"ok","timestamp":1664693881953,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"pLhwSg0WsKF6","outputId":"df872c77-4e79-4fd4-e1c3-543edf7642d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Load word dictionary\n","The size of word2idx is 6093\n"]}],"source":["# build word2idx, idx2word (생성시에 사용)\n","# train : 학습데이터셋\n","# word -> index 매핑 사전\n","word2idx = build_dict(train, THRESHOLD)\n","# index -> word 매핑 사전\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# 사전 사이즈\n","vocab_size = len(word2idx)\n","print(\"The size of word2idx is {}\".format(len(word2idx)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1664693941982,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"92WdPXUiHqdP","outputId":"7715fd52-415b-417f-94aa-28764c906ce7"},"outputs":[{"output_type":"stream","name":"stdout","text":["<pad>\n","저\n","비례\n"]}],"source":["print(idx2word[0])\n","print(idx2word[100])\n","print(idx2word[6092])\n"]},{"cell_type":"markdown","metadata":{"id":"7Rmp9XJCsKGK"},"source":["## Step 4: Prepare data for Model\n","\n","모델의 학습을 위해 학습 데이터를 word dictionary 를 이용하여 index 로 변환합니다. 그리고 학습 데이터를 mini-batch 단위로 준비합니다.\n","\n","- `batch_iter`: 학습 과정에서 iteration 돌 때마다, 배치 단위의 데이터를 불러오는 과정을 위한 함수\n","- `batch_dataset`: mini-batch 단위 학습 데이터를 만들어주는 함수\n","    - 모델 학습의 efficiency 를 위해 **MAX_LEN** 만큼 데이터 자르기\n","    - 처음 보는 단어는 **unk** 토큰으로 채우기\n","    - utterance 문장의 뒤에 **eos** 토큰 더하기\n","    - response 문장의 앞/뒤에 **sos/eos** 토큰 더하기\n","    - **MAX_LEN** 까지 남은 부분은 **pad** 토큰 채우기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q90Evxu7sKGK"},"outputs":[],"source":["def batch_iter(data, batch_size): \n","    num_batches_per_epoch = int(len(data) / batch_size)\n","    # data : 현재 스트링 타입 -> array 타입으로 저장 (이후에 텐서로 변환된다)\n","    data = np.array(data)\n","    \n","    # num_batches_per_epoch = 332 \n","    for batch_idx in range(num_batches_per_epoch):\n","        # start_idx = 0, 32, 64, ...\n","        start_idx = batch_idx * batch_size\n","        # ebd_idx = 32, 64, 96, ...\n","        end_idx = min((batch_idx + 1) * batch_size, len(data))\n","        enc_data = list()   # 인코더 데이터\n","        dec_data = list()   # 디코더 데이터\n","        \n","        for each in data[start_idx:end_idx]:\n","            enc_data.append(each[0])  # utterence\n","            dec_data.append(each[1])  # response\n","\n","        yield enc_data, dec_data  # 인코더 데이터, 디코더 데이터 반환하는 것을 반복\n","        \n","# 인코더 데이터, 디코더 데이터 -> index로 바꾼 후 array 타입으로 반환하는 함수\n","def batch_dataset(batch_x, batch_y, word2idx):\n","    # batch input & target\n","    # map(함수, 리스트) : 리스트로부터 원소를 하나씩 꺼내서 함수를 적용시킨 다음, 그 결과를 새로운 리스트에 넣는다\n","    batch_x = list(map(lambda x: x[:MAX_LEN], batch_x)) # utterence 문장을 하나씩 꺼내서 MAX_LEN 사이즈로 자른다\n","    batch_y = list(map(lambda x: x[:MAX_LEN], batch_y)) # response 문장을 하나씩 꺼내서 MAX_LEN 사이즈로 자른다\n","\n","    # utterence 문장, response 문장 -> index로 변환\n","    # word2idx 딕셔너리에서 찾는 단어가 없을 땐 디폴트값인 '<unk>'의 index를 반환\n","    batch_x = list(map(lambda x: [word2idx.get(each, word2idx['<unk>']) for each in x], batch_x))\n","    batch_y = list(map(lambda x: [word2idx.get(each, word2idx['<unk>']) for each in x], batch_y))\n","                        \n","    batch_enc_input = list(map(lambda x: list(x) + [word2idx['<eos>']], batch_x))            \n","    batch_dec_target = list(map(lambda x: [word2idx['<sos>']] + list(x) + [word2idx['<eos>']], batch_y))\n","\n","    # MAX_LEN보다 짧은 문장이라면 <pad> 채워넣기        \n","    batch_enc_input = list(map(lambda x: list(x) + (MAX_LEN+1 - len(x)) * [word2idx['<pad>']], batch_enc_input))         \n","    batch_dec_target = list(map(lambda x: list(x) + (MAX_LEN+2 - len(x)) * [word2idx['<pad>']], batch_dec_target))\n","    \n","    batch_enc_input = np.array(batch_enc_input) # integer 리스트 --> array로 변환\n","    batch_dec_target = np.array(batch_dec_target)\n","    \n","    return batch_enc_input, batch_dec_target # integer 값이 저장된 array 반환\n"]},{"cell_type":"markdown","metadata":{"id":"7xJRIVVssKGM"},"source":["## Step 5: Building seq2seq model with attention mechanism\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9miahyT192bp"},"outputs":[],"source":["# Encoder RNN 클래스 정의, nn.Module을 상속\n","class EncoderRNN(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, n_layers=1, dropout=0):\n","        super(EncoderRNN, self).__init__() # 부모 클래스 nn.Module을 초기화\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.embedding = nn.Embedding(vocab_size, hidden_size) # vocab size x 히든사이즈 matrix, 랜덤값으로 초기화\n","        \n","        # Initialize LSTM with bidirectional\n","        # the input_size and hidden_size params are both set to 'hidden_size'\n","        # because our input size is a word embedding with number of features == hidden_size\n","        # nn.LSTM (input_size, hidden_size, n_layers, bidirectional, droupout, batch_first)\n","        # batch_first가 True -> 텐서 1번째 차원이 배치 사이즈인 것을 알림\n","        # input tensors, output tensors -> (batch_size, seq_length, embed_size)\n","        # Note that this does not apply to hidden or cell states. \n","        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n","\n","    def forward(self, enc_input):\n","        # Convert word indexed to embeddings (mapping discrete tokens to continuous space)\n","        # embedded shape == (batch_size, enc_max_len, embed_size)\n","        embedded = self.embedding(enc_input)\n","       \n","        # Forward pass through RNN module\n","        # if bidirectional, outputs shape == (batch_size, seq_length, hidden_size*2)\n","        # if not bidirectional, outputs shape == (batch_size, seq_length, hidden_size)\n","        # hidden shape == (num_directions * num_layers, batch_size, hidden_size)\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","\n","        \n","        # output of shape (batch_size, seq_length, num_directions * hidden_size)\n","        # h of shape (num_layers * num_directions, batch, hidden_size)\n","        # c of shape (num_layers * num_directions, batch, hidden_size)\n","        return outputs, (hidden, cell)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFFfPyraX90I"},"outputs":[],"source":["# Luong attention layer\n","class Attention(nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attention, self).__init__()\n","        self.method = method\n","        # method: indicator that determines the score function \n","        \n","        self.hidden_size = hidden_size          \n","\n","    def dot_score(self, hidden, encoder_output):        \n","        # attention dot score function (Luong)\n","        # attention score shape == (batch_size, dec_max_len)\n","        # sum: returns the sum of each row of the input tensor in the given dimension dim\n","        return torch.sum(hidden * encoder_output, dim=2) # 텐서의 dimension 2가 squeeze\n","\n","    def forward(self, hidden, encoder_outputs):\n","        attn_weights = self.dot_score(hidden, encoder_outputs)\n","        \n","        # Transpose max_length and batch_size dimensions\n","        attention = attn_weights\n","\n","        # Normalize attention to weights in range 0 to 1\n","        return F.softmax(attention, dim=1).unsqueeze(1) # attention을 3D shape로 변환 (resize to 1 x 1 x seq_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O41_EgTtX90L"},"outputs":[],"source":["# Decoder Model\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, attn_model, hidden_size, vocab_size, n_layers=1, dropout=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        \n","        self.attn_model = attn_model # 'dot' 프로덕트\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","        \n","        # Define layers\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","             \n","        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, bidirectional=False, num_layers=1, batch_first=True)\n","        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n","\n","        self.attn = Attention(self.attn_model, self.hidden_size)\n","    \n","    def forward(self, dec_input, hidden, cell, encoder_outputs):\n","        # Note: we run this one step (word) at a time\n","        \n","        # Get embedding of current input word\n","        # embedded shape == (batch_size, 1, embed_size)\n","        embedded = self.embedding(dec_input)\n","    \n","        # Forward through unidirectional LSTM\n","        # output shape == (batch_size, 1, hidden_size)\n","        # hidden shape == (num_directions * num_layers, batch_size, hidden_size)        \n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","\n","        # Calculate attention weights from the current LSTM output\n","        # 현 시점의 디코더 hidden(Query)과 인코더의 모든 hidden(Key)을 dot product -> softmax -> attention weights\n","        # attention weights shape == (batch_size, 1, dec_max_len)\n","        attn_weights = self.attn(output, encoder_outputs) \n","\n","        # Multiply attention weights to encoder outputs to get new \"weighted sum(가중합)\" context vector\n","        # context vector shape == (batch_size, 1, hidden_size)\n","        # torch.bmm 함수는 배치 행렬 곱(Batch Matrix Multiplication, BMM)을 수행하는 함수로서\n","        # 뒤의 두 개 차원에 대해 행렬 곱을 수행함 \n","        # attn_weights X 인코더 모든 hidden states -> context \n","        context = attn_weights.bmm(encoder_outputs)  \n","        \n","        # Concatenate weighted context vector and LSTM output using Luong\n","        # output shape == (batch_size, 1, hidden_size)\n","        # context shape == (batch_size, 1, hidden_size) \n","        # concat_input shape == (batch_size, hidden_size * 2)\n","        # concat_output shape == (batch_size, hidden_size)\n","        output = output.squeeze(1)    # dimension 1의 차원을 제거\n","        context = context.squeeze(1)  # dimension 1의 차원을 제거\n","        concat_input = torch.cat((output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        \n","        # Predict next word using Luong\n","        # output shape == (batch_size, vocab_size)\n","        output = self.out(concat_output)\n","        output = F.log_softmax(output, dim=1) # dimension 1의 값을 softmax (dimension 0는 batch_size)\n","        \n","        # Return output and final hidden state\n","        return output, hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"fGdoo9pvsKGT"},"source":["## Step 6: Define the optimizer and the loss function\n","\n","- optimizer: Adam 사용\n","- loss function: negative log likelihood 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7325,"status":"ok","timestamp":1664693969658,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"CvT9E4hVC4RS","outputId":"f82edf7e-6c3e-4b75-a166-ac0f32846569"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"stream","name":"stdout","text":["Models are built and ready to go!\n"]}],"source":["# Initialize encoder & decoder models\n","# model.cuda() : model의 모든 parameter를 GPU에 loading\n","encoder = EncoderRNN(vocab_size, enc_hidden_size, encoder_n_layers, dropout).cuda() \n","decoder = AttnDecoderRNN(attn_model, dec_hidden_size, vocab_size, decoder_n_layers, dropout).cuda()\n","\n","print(\"Models are built and ready to go!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoqcWpJ492hp"},"outputs":[],"source":["encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","criterion= nn.NLLLoss()"]},{"cell_type":"markdown","metadata":{"id":"HROxutIOqcNf"},"source":["## Step 7: Training with evaluation\n","\n","- evaluation: training with evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iSOx0Boil9q"},"outputs":[],"source":["def evaluate(encoder, decoder, val_batches, device, word2idx):\n","    #evaluating the validation loss\n","    total_loss = 0.\n","    \n","    for batch_idx, (batch_x, batch_y) in enumerate(val_batches):\n","        if batch_idx == 1:\n","            break\n","        # 인코더 데이터, 디코더 데이터 -> index로 바꾼 후 array 타입으로 반환하는 함수    \n","        batch_enc_input, batch_dec_target = batch_dataset(batch_x, batch_y, word2idx)\n","        \n","        # long 타입의 텐서를 생성해서 GPU에 로딩\n","        # batch_enc_input : utterence\n","        # batch_dec_target : response\n","        batch_enc_input = torch.tensor(batch_enc_input, dtype = torch.long, device='cuda')\n","        batch_dec_target = torch.tensor(batch_dec_target, dtype = torch.long, device='cuda')\n","        \n","        loss = 0.\n","        \n","        with torch.no_grad():\n","            # Forward pass through encoder\n","            encoder_outputs, (encoder_hidden, encoder_cell) = encoder(batch_enc_input)\n","\n","            # torch.cat(tensors, dim=0) : \n","            # Concatenates the given sequence of tensors in the given dimension. \n","            # dim: the dimension over which the tensors are concatenated          \n","            eh = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=1).unsqueeze(0)\n","            ec = torch.cat((encoder_cell[0], encoder_cell[1]), dim=1).unsqueeze(0)\n","            \n","            # Create initial decoder input (start with <sos> tokens for each sentence)\n","            dec_input = torch.tensor([word2idx['<sos>']] * batch_size, dtype=torch.long, device=device)\n","            dec_input = dec_input.unsqueeze(1)                  # dec_input shape : (? , ?)\n","\n","            # Set initial decoder hidden state to the encoder's final hidden state\n","            decoder_hidden = encoder_hidden[:decoder.n_layers]  # encoder_hidden[0]\n","\n","            # Tensor.size(dim=None) : If dim is specified, returns the size of that dimension.\n","            # 즉, response의 max_len을 반환            \n","            for t in range(1, batch_dec_target.size(1)):\n","                decoder_output, eh, ec = decoder(dec_input, eh, ec, encoder_outputs)\n","\n","                # Calculate and accumulate loss\n","                loss += criterion(decoder_output, batch_dec_target[:, t])\n","\n","                # Teacher forcing: next input is current target\n","                dec_input = batch_dec_target[:, t].unsqueeze(1)\n"," \n","            # print (batch_dec_target.size(1))\n","            # item() method extracts the loss’s value as a Python float    \n","            batch_loss = loss.item() / int(batch_dec_target.size(1))\n","            total_loss += batch_loss\n","        \n","    # total_loss를 batch 갯수로 나누어 loss 평균값을 반환\n","    return total_loss / (batch_idx + 1)    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158567,"status":"ok","timestamp":1664694142912,"user":{"displayName":"김나리","userId":"01119342099285995005"},"user_tz":-540},"id":"D9JrypFyUfp9","outputId":"cea00169-28ad-47e8-ff99-1605122e75d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[num_batches_per_epoch] 332\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  after removing the cwd from sys.path.\n"]},{"output_type":"stream","name":"stdout","text":["[epoch 1 | step 100/332] loss: 1.5400 (Avg. 2.097175) PPL: 4.6645 (Avg. 70.6520)\n","[epoch 1 | step 200/332] loss: 1.4916 (Avg. 1.789732) PPL: 4.4444 (Avg. 37.5489)\n","[epoch 1 | step 300/332] loss: 1.2401 (Avg. 1.641250) PPL: 3.4558 (Avg. 26.3207)\n","[epoch 1] loss: 0.6323\n","[epoch 2 | step 100/332] loss: 1.1720 (Avg. 1.234736) PPL: 3.2284 (Avg. 3.4578)\n","[epoch 2 | step 200/332] loss: 1.1783 (Avg. 1.187921) PPL: 3.2490 (Avg. 3.3034)\n","[epoch 2 | step 300/332] loss: 0.9995 (Avg. 1.152238) PPL: 2.7168 (Avg. 3.1894)\n","[epoch 2] loss: 0.5699\n","[epoch 3 | step 100/332] loss: 0.9477 (Avg. 1.012397) PPL: 2.5797 (Avg. 2.7644)\n","[epoch 3 | step 200/332] loss: 0.9286 (Avg. 0.972499) PPL: 2.5309 (Avg. 2.6584)\n","[epoch 3 | step 300/332] loss: 0.8084 (Avg. 0.939317) PPL: 2.2443 (Avg. 2.5727)\n","[epoch 3] loss: 0.5435\n","[epoch 4 | step 100/332] loss: 0.7353 (Avg. 0.805989) PPL: 2.0862 (Avg. 2.2462)\n","[epoch 4 | step 200/332] loss: 0.7085 (Avg. 0.769010) PPL: 2.0309 (Avg. 2.1659)\n","[epoch 4 | step 300/332] loss: 0.6138 (Avg. 0.736796) PPL: 1.8475 (Avg. 2.0981)\n","[epoch 4] loss: 0.5349\n","[epoch 5 | step 100/332] loss: 0.5463 (Avg. 0.608154) PPL: 1.7269 (Avg. 1.8407)\n","[epoch 5 | step 200/332] loss: 0.5334 (Avg. 0.576488) PPL: 1.7047 (Avg. 1.7841)\n","[epoch 5 | step 300/332] loss: 0.4520 (Avg. 0.548204) PPL: 1.5714 (Avg. 1.7350)\n","[epoch 5] loss: 0.5365\n","[epoch 6 | step 100/332] loss: 0.3750 (Avg. 0.436837) PPL: 1.4550 (Avg. 1.5495)\n","[epoch 6 | step 200/332] loss: 0.3927 (Avg. 0.412810) PPL: 1.4809 (Avg. 1.5131)\n","[epoch 6 | step 300/332] loss: 0.3502 (Avg. 0.391848) PPL: 1.4193 (Avg. 1.4820)\n","[epoch 6] loss: 0.5562\n","[epoch 7 | step 100/332] loss: 0.2493 (Avg. 0.301056) PPL: 1.2831 (Avg. 1.3521)\n","[epoch 7 | step 200/332] loss: 0.2904 (Avg. 0.283805) PPL: 1.3370 (Avg. 1.3291)\n","[epoch 7 | step 300/332] loss: 0.2427 (Avg. 0.269440) PPL: 1.2747 (Avg. 1.3103)\n","[epoch 7] loss: 0.5673\n","[epoch 8 | step 100/332] loss: 0.1563 (Avg. 0.200125) PPL: 1.1692 (Avg. 1.2220)\n","[epoch 8 | step 200/332] loss: 0.1802 (Avg. 0.186032) PPL: 1.1974 (Avg. 1.2049)\n","[epoch 8 | step 300/332] loss: 0.1657 (Avg. 0.176366) PPL: 1.1802 (Avg. 1.1934)\n","[epoch 8] loss: 0.5854\n","[epoch 9 | step 100/332] loss: 0.1011 (Avg. 0.126301) PPL: 1.1063 (Avg. 1.1348)\n","[epoch 9 | step 200/332] loss: 0.1219 (Avg. 0.116549) PPL: 1.1296 (Avg. 1.1238)\n","[epoch 9 | step 300/332] loss: 0.1084 (Avg. 0.110529) PPL: 1.1145 (Avg. 1.1171)\n","[epoch 9] loss: 0.6257\n","[epoch 10 | step 100/332] loss: 0.0689 (Avg. 0.079170) PPL: 1.0713 (Avg. 1.0825)\n","[epoch 10 | step 200/332] loss: 0.0775 (Avg. 0.072638) PPL: 1.0806 (Avg. 1.0754)\n","[epoch 10 | step 300/332] loss: 0.0685 (Avg. 0.068847) PPL: 1.0709 (Avg. 1.0714)\n","[epoch 10] loss: 0.6182\n","CPU times: user 2min 36s, sys: 497 ms, total: 2min 36s\n","Wall time: 2min 38s\n"]}],"source":["%%time\n","\n","teacher_forcing_ratio = 1.0\n","LOG_INTERVAL = 100\n","\n","num_batches_per_epoch = int(len(train) / batch_size)\n","print(\"[num_batches_per_epoch] {}\".format(num_batches_per_epoch))\n","\n","# Training Epoch: 10\n","for epoch in range(max_epochs):\n","\n","    total_loss = 0.\n","    total_ppl = 0.\n","    print_loss = 0.\n","    \n","    # batch_iter : 학습 과정에서 iteration 돌 때마다, 배치 단위의 데이터를 불러오는 함수\n","    train_batches = batch_iter(train, batch_size)\n","    \n","    # Set models as training mode\n","    encoder.train()\n","    decoder.train()\n","    \n","    # Training loop\n","    for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n","      \n","        batch_enc_input, batch_dec_target = batch_dataset(batch_x, batch_y, word2idx)\n","        \n","        # Initialize variables\n","        loss = 0.\n","        \n","        # zero gradients\n","        # Iteration이 한번 끝나면 gradients를 항상 0으로 reset해야 함 \n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        # batch_enc_input shape : (batch_size, seq_length)\n","        # batch_dec_target shape : (batch_size, seq_length)\n","        batch_enc_input = torch.tensor(batch_enc_input, dtype = torch.long, device='cuda')\n","        batch_dec_target = torch.tensor(batch_dec_target, dtype = torch.long, device='cuda')\n","\n","        # Forward pass through encoder\n","        encoder_outputs, (encoder_hidden, encoder_cell) = encoder(batch_enc_input)\n","                 \n","        eh = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=1).unsqueeze(0)\n","        ec = torch.cat((encoder_cell[0], encoder_cell[1]), dim=1).unsqueeze(0)\n","        \n","        # Create initial decoder input (start with <sos> tokens for each sentence)\n","        dec_input = torch.tensor([word2idx['<sos>']] * batch_size, dtype=torch.long, device=device)\n","        dec_input = dec_input.unsqueeze(1)    # dec_input shape : (batch_size, 1)\n","\n","        # Set initial decoder hidden state to the encoder's final hidden state\n","        decoder_hidden = encoder_hidden[:decoder.n_layers]\n","\n","        # Determine if we are using teacher forcing this iteration\n","        # random.random() -> 0~1 사이의 난수 생성\n","        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","        \n","        if use_teacher_forcing:\n","            for t in range(1, batch_dec_target.size(1)):\n","                \n","                decoder_output, eh, ec = decoder(dec_input, eh, ec, encoder_outputs)\n","                \n","                # Calculate and accumulate loss\n","                loss += criterion(decoder_output, batch_dec_target[:, t])\n","\n","                # Teacher forcing: next input is current target\n","                # batch_dec_target의 전체 row의 t번째 값(정답)을 dec_input에 준다\n","                # dec_input shape : (batch_size, 1)\n","                dec_input = batch_dec_target[:, t].unsqueeze(1)                \n","        else:\n","            for t in range(1, batch_dec_target.size(1)):\n","                decoder_output, eh, ec = decoder(dec_input, eh, ec, encoder_outputs)\n","            \n","                # No teacher forcing: next input is decoder's own current output\n","                _, topi = decoder_output.topk(1)\n","            \n","                dec_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","                dec_input = dec_input.to(device)\n","            \n","                # Calculate and accumulate loss\n","                loss += criterion(decoder_output, batch_dec_target[:, t])\n","            \n","        # item() method extracts the loss’s value as a Python float     \n","        batch_loss = (loss.item() / int(batch_dec_target.size(1)))\n","        batch_ppl = np.exp(batch_loss)\n","        \n","        total_loss += batch_loss\n","        total_ppl += batch_ppl\n","        print_loss += loss.item()\n","        \n","        # Perform backpropagation\n","        loss.backward()\n","            \n","        # Adjust/Update model weights\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","        \n","        if (batch_idx + 1) % LOG_INTERVAL == 0:\n","            print(\"[epoch {} | step {}/{}] loss: {:.4f} (Avg. {:4f}) PPL: {:.4f} (Avg. {:.4f})\".format(epoch+1,\n","                                                                                                       batch_idx+1,\n","                                                                                                       num_batches_per_epoch,\n","                                                                                                       batch_loss, total_loss/(batch_idx + 1),\n","                                                                                                       batch_ppl, total_ppl/(batch_idx + 1)))\n","\n","    # check validation\n","    val_batches = batch_iter(test, batch_size)   \n","\n","    encoder.cuda().eval()\n","    decoder.cuda().eval()\n","    val_loss = evaluate(encoder, decoder, val_batches, device, word2idx)\n","    print(\"[epoch {}] loss: {:.4f}\".format(epoch+1, val_loss))    \n","    \n","    \n","#모델 저장\n","torch.save(encoder, dirpath + 'encoder_' + str(epoch + 1) + '.pt')\n","torch.save(decoder, dirpath + 'decoder_' + str(epoch + 1) + '.pt')\n","#total_loss_per_epoch = (total_loss / (batch_idx + 1))"]},{"cell_type":"markdown","metadata":{"id":"OP9aAgumu1tV"},"source":["##Step 7: Test the chatbot model\n","- inference: 질문을 임베딩 벡터로 변환해 모델에 입력하고 그에 따른 출력값을 문장으로 변환\n","- unfiltering: 특수문자를 단어들로부터 제거하여 원래 단어 스트링들만 추출"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tr1JSMz2l6Jq"},"outputs":[],"source":["def inference(question):\n","  #질문을 임베딩 벡터로 변환해 모델에 입력하고 그에 따른 출력값을 문장으로 변환\n","\n","  #질문을 idx 들로 변환한다.\n","  sentence = tokenizer.tokenize(question)\n","  sentence.append('<eos>')\n","  sentence = sentence + (MAX_LEN+1 - len(sentence)) * ['<pad>']\n","  sentence_idx = [[word2idx[word] if word in word2idx else UNK_token for word in sentence]] #1 for unk_token\n","  \n","  #입력 문장의 idx 를 벡터로 변환하여 인코더의 input 으로\n","  enc_input = torch.tensor(sentence_idx , device=device)\n","\n","  encoder_outputs, (encoder_hidden, encoder_cell) = encoder(enc_input)\n","  eh = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=1)\n","  ec = torch.cat((encoder_cell[0], encoder_cell[1]), dim=1)\n","  decoder_input = torch.tensor([word2idx['<sos>']], dtype=torch.long, device=device)\n","\n","  #인코더의 input 과 <sos> 토큰을 decoder에 입력하여 다음 단어 예측 \n","  decoded_words = []\n","  for di in range(MAX_LEN):\n","    decoder_output, eh, ec = decoder(decoder_input, eh, ec, encoder_outputs)\n","\n","    #가장 확률이 높은 단어 하나를 예측하고, 이전 입력에 연결하여 다음 예측에 사용\n","    topv, topi = decoder_output.data.topk(1)\n","    if topi.item() == EOS_token or topi.item() == PAD_token:\n","        decoded_words.append('<eos>')\n","        break\n","    else:\n","        decoded_words.append(idx2word[topi.item()])\n","    decoder_input = torch.tensor([topi], device=device)\n","    \n","  print('text:', decoded_words)\n","  return unfiltering(decoded_words)\n","  print(decoded_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH2ZHaGjGISP"},"outputs":[],"source":["# 특수문자를 단어들로부터 제거\n","def unfiltering(text):\n","  origin_text = ''\n","  for words in text[:-2]:\n","    origin_text += words + ' '\n","\n","  return origin_text.replace(' ##' ,'')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tc8RIhMlK7iN","outputId":"038e8821-fbf1-4771-ffdf-8ed7f93c54e9","executionInfo":{"status":"ok","timestamp":1664694329583,"user_tz":-540,"elapsed":117236,"user":{"displayName":"김나리","userId":"01119342099285995005"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["챗봇과 대화\n","문장을 입력하세요: 오늘 비가 내려\n","text: ['당신', '##의', '삶', '##을', '응원', '##해', '드릴', '수', '있', '##어요', '##라고', '감히', '말', '##해', '봅니다', '.', '<eos>']\n","Bot: 당신의 삶을 응원해 드릴 수 있어요라고 감히 말해 봅니다 \n","문장을 입력하세요: 승주 결혼 축하해\n","text: ['무엇', '##을', '해줘', '##서', '잘', '##하', '##는', '것', '##이', '아니', '##에', '##요', '.', '<eos>']\n","Bot: 무엇을 해줘서 잘하는 것이 아니에요 \n","문장을 입력하세요: 우리 모두 화이팅하자\n","text: ['좋', '##은', '느낌', '##들', '##의', '사랑', '##이', '##네', '##요', '.', '<eos>']\n","Bot: 좋은 느낌들의 사랑이네요 \n","문장을 입력하세요: 예쁜 사랑하세요\n","text: ['진정', '##으로', '사랑', '##한다', '##면', '그럴', '##수도', '있', '##을', '##거', '같', '##아', '##요', '.', '<eos>']\n","Bot: 진정으로 사랑한다면 그럴수도 있을거 같아요 \n","문장을 입력하세요: 오늘은 기분이 울적해요\n","text: ['거리', '##를', '걸어', '##보', '##세요', '.', '<eos>']\n","Bot: 거리를 걸어보세요 \n","문장을 입력하세요: /q\n","Quitting chat..\n"]}],"source":["print(\"챗봇과 대화\")\n","while (1):\n","  user_input = input(\"문장을 입력하세요: \")\n","  user_input = str(user_input)\n","\n","  if user_input == '/q':\n","    print(\"Quitting chat..\")\n","    break;\n","  else:\n","    print(\"Bot: \" + str(inference(user_input)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRCcOACkpKyp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"533032f1b5f24db6ace9455a6c90ab30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7688817835fc45e4b4359fb084690a55","IPY_MODEL_52ee3528ab6e4d4e915cb23ee6aae2ab","IPY_MODEL_5dac1d88aad2481ca4adbba8d4222fe1"],"layout":"IPY_MODEL_fd8c2f752e734df2ad37a38eceef14d4"}},"7688817835fc45e4b4359fb084690a55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf9535bde93488b80257f330524d49c","placeholder":"​","style":"IPY_MODEL_b89f33b3d2804de29c7b32dcad5ac1ed","value":"Downloading: 100%"}},"52ee3528ab6e4d4e915cb23ee6aae2ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6803f37fa0e94202b860f4ab1863e388","max":373,"min":0,"orientation":"horizontal","style":"IPY_MODEL_881af838ff7941419d4ff51a63304eb2","value":373}},"5dac1d88aad2481ca4adbba8d4222fe1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74e9dc27041a432da4677d0c46783058","placeholder":"​","style":"IPY_MODEL_02a656ded174429ea65d684754331709","value":" 373/373 [00:00&lt;00:00, 14.7kB/s]"}},"fd8c2f752e734df2ad37a38eceef14d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cf9535bde93488b80257f330524d49c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b89f33b3d2804de29c7b32dcad5ac1ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6803f37fa0e94202b860f4ab1863e388":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"881af838ff7941419d4ff51a63304eb2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74e9dc27041a432da4677d0c46783058":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02a656ded174429ea65d684754331709":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2c6fd045d674ebaa63de32ab8a8ebe3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7df7c388ff754ffe8ee02bd45ead6b33","IPY_MODEL_ceed8a860d4f49ba95e6e925ca4ff525","IPY_MODEL_b1d465d2e0c34b4ba7cc8c000ed2b1ff"],"layout":"IPY_MODEL_616c2c157e8545028dbb7ae548237220"}},"7df7c388ff754ffe8ee02bd45ead6b33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6103b5b0fc49455581a46af9d490e2ca","placeholder":"​","style":"IPY_MODEL_bd72994f856949d3847187eb37bb48cb","value":"Downloading: 100%"}},"ceed8a860d4f49ba95e6e925ca4ff525":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d712fa5fb50b4781a88c543c8a30ef87","max":241171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74f494f65d7a4c0da5cee0d76448be5e","value":241171}},"b1d465d2e0c34b4ba7cc8c000ed2b1ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cba3fad142394ce89113de5348920709","placeholder":"​","style":"IPY_MODEL_c94559938a2e4edf95bd26b46f9eb1ce","value":" 241k/241k [00:00&lt;00:00, 3.12MB/s]"}},"616c2c157e8545028dbb7ae548237220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6103b5b0fc49455581a46af9d490e2ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd72994f856949d3847187eb37bb48cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d712fa5fb50b4781a88c543c8a30ef87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f494f65d7a4c0da5cee0d76448be5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cba3fad142394ce89113de5348920709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94559938a2e4edf95bd26b46f9eb1ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8395ae1616df489ba33f6c219956ad24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab502e97b25a46cb96184522098031c8","IPY_MODEL_d4f273b2a37d47a0ac890c2c2d709830","IPY_MODEL_b1e859c705fb44f9a4bae06d99746cce"],"layout":"IPY_MODEL_167066d9340e42e59aaa7c6ae7a78c54"}},"ab502e97b25a46cb96184522098031c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cffe96143d3e46e399c9038a84b4c5e2","placeholder":"​","style":"IPY_MODEL_ee13ced332ad4bb0a1bb5f2fc082c5f6","value":"Downloading: 100%"}},"d4f273b2a37d47a0ac890c2c2d709830":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_078c6392a0a648918be6eb4659526b34","max":491774,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b19a723ed0c84695beeae8977bb5b921","value":491774}},"b1e859c705fb44f9a4bae06d99746cce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84f243f21ad44b2797f8469d92adc668","placeholder":"​","style":"IPY_MODEL_97acd49bfed14cb28c5253eaa14b8525","value":" 492k/492k [00:00&lt;00:00, 4.98MB/s]"}},"167066d9340e42e59aaa7c6ae7a78c54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cffe96143d3e46e399c9038a84b4c5e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee13ced332ad4bb0a1bb5f2fc082c5f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"078c6392a0a648918be6eb4659526b34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b19a723ed0c84695beeae8977bb5b921":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84f243f21ad44b2797f8469d92adc668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97acd49bfed14cb28c5253eaa14b8525":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"461c89689a644734aaa755a25da3e495":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06f7a6102787462b9d6dfe812bd56708","IPY_MODEL_e1cf94f1f3ac4d2fbe6f43cbb82af661","IPY_MODEL_d1e14b7196f2474681dc30eb6c0a514c"],"layout":"IPY_MODEL_80e077170aac4703adaab7e9302ba2fd"}},"06f7a6102787462b9d6dfe812bd56708":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb8f306cb0cd4a9189ca2d42df19186b","placeholder":"​","style":"IPY_MODEL_1c3189e110e44462b31049e039c50ea0","value":"Downloading: 100%"}},"e1cf94f1f3ac4d2fbe6f43cbb82af661":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e669c2773b2547a5a4fefdd2485f6cf6","max":169,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35d91b3049c9408397cb45dc85e36d6f","value":169}},"d1e14b7196f2474681dc30eb6c0a514c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd734ea368f94372b00b2fd4b92f6d13","placeholder":"​","style":"IPY_MODEL_dba05c32c44d4417a5f9859c80abbb3a","value":" 169/169 [00:00&lt;00:00, 6.41kB/s]"}},"80e077170aac4703adaab7e9302ba2fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb8f306cb0cd4a9189ca2d42df19186b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c3189e110e44462b31049e039c50ea0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e669c2773b2547a5a4fefdd2485f6cf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35d91b3049c9408397cb45dc85e36d6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd734ea368f94372b00b2fd4b92f6d13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dba05c32c44d4417a5f9859c80abbb3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}